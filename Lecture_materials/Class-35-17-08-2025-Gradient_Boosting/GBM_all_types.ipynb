{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d190c69",
   "metadata": {},
   "source": [
    "# Gradient-Boosting Classifiers\n",
    "\n",
    "**Dataset:** UCI / Kaggle Bank Marketing (`bank-additional-full.csv`). ~41,000 rows, mixed types.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- Section 1: dataset, EDA, preprocessing (categorical, numerical, boolean)\n",
    "- Section 2: sklearn GradientBoostingClassifier (train, tune, feature importance)\n",
    "- Section 3: XGBoost (train, tune, feature importance + Optuna Bayesian tuning example)\n",
    "- Section 4: LightGBM (train, tune, feature importance)\n",
    "- Section 5: CatBoost (train, tune, feature importance)\n",
    "\n",
    "**Notes:**\n",
    "- Download `bank-additional-full.csv` from UCI (https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) and place it next to this notebook.\n",
    "- Hyperparameter Tuning may take time depending on your machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929955f",
   "metadata": {},
   "source": [
    "**Installation Note:**\n",
    "\n",
    "If you haven't installed the packages like xgboost, catboost, and lightGBM, use the following command to install them all in your virtual environment.\n",
    "\n",
    "```shell\n",
    "pip install xgboost catboost lightgbm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4917b68",
   "metadata": {},
   "source": [
    "## Section 1 — Load data, EDA and preprocessing\n",
    "\n",
    "We use the Bank Marketing dataset (campaign results of a Portuguese bank). Target: `y` (yes/no) — converted to binary. The dataset contains numeric, categorical and binary features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fa725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DATA_PATH = \"./bank-additional-full.csv\"  # place the file next to this notebook\n",
    "df = pd.read_csv(DATA_PATH, sep=';')\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df['y'].value_counts(normalize=True))\n",
    "print(\"\\nSample missing values per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head())\n",
    "\n",
    "# Identify types\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# In this dataset some \"categorical\" may be encoded as object; boolean-like cols are within object too.\n",
    "print(f\"\\nCategorical cols ({len(cat_cols)}): {cat_cols}\")\n",
    "print(f\"Numeric cols ({len(num_cols)}): {num_cols}\")\n",
    "\n",
    "# Convert target to 0/1\n",
    "df['target'] = (df['y'] == 'yes').astype(int)\n",
    "df.drop(columns=['y'], inplace=True)\n",
    "\n",
    "# Basic numeric histograms\n",
    "plt.figure(figsize=(10,4))\n",
    "df[num_cols].hist(bins=30, figsize=(12,6))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cda7e",
   "metadata": {},
   "source": [
    "### Preprocessing plan\n",
    "\n",
    "- **Categorical**: One-hot encoding for sklearn GB; for other methods we will use Ordinal/Label encoding or let libraries handle categoricals (CatBoost accepts categorical features natively).\n",
    "- **Numeric**: median imputation (though this dataset has no missing numeric values) and optional scaling for sklearn GB.\n",
    "- **Train/test split**: stratified split on `target`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split and create preprocessing pipelines\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "TARGET = 'target'\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# recompute types on train\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# preprocessing for sklearn GB (one-hot + scaling numeric)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor_sklearn = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# preprocessing for other models (ordinal encode categoricals, scale not required)\n",
    "ordinal_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols)\n",
    "])\n",
    "\n",
    "# For CatBoost we'll pass raw data and list of categorical column names (it handles encoding)\n",
    "print(\"Numeric cols:\", len(num_cols), \"Categorical cols:\", len(cat_cols))\n",
    "\n",
    "print(\"Numerical columns: \", num_cols)\n",
    "print(\"Categorical columns: \", cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8a559",
   "metadata": {},
   "source": [
    "## Section 2 — sklearn `GradientBoostingClassifier`\n",
    "\n",
    "Gradient boosting builds an additive model of weak learners (trees). At iteration $m$ the model is:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\alpha h_m(x)$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $h_m$ is the new tree fit to the negative gradient.\n",
    "\n",
    "**Documentation:** \n",
    "\n",
    "- [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "- [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "\n",
    "**Main hyperparameters:** `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `max_features`, `min_samples_leaf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c826f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline sklearn GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "gb_pipe = make_pipeline(preprocessor_sklearn, GradientBoostingClassifier(\n",
    "    n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42\n",
    "))\n",
    "gb_pipe.fit(X_train, y_train)\n",
    "proba_gb = gb_pipe.predict_proba(X_test)[:,1]\n",
    "print(f\"Sklearn GB Test AUC: {roc_auc_score(y_test, proba_gb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd59050",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (RandomizedSearchCV)\n",
    "\n",
    "We use `RandomizedSearchCV` over reasonable ranges. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV for sklearn GB\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distributions = {\n",
    "    'gradientboostingclassifier__n_estimators': randint(100, 500),\n",
    "    'gradientboostingclassifier__learning_rate': uniform(0.01, 0.2),\n",
    "    'gradientboostingclassifier__max_depth': randint(2, 6),\n",
    "    'gradientboostingclassifier__subsample': uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "# create pipeline with named step so params can be referenced\n",
    "from sklearn.pipeline import Pipeline\n",
    "gb_pipeline = Pipeline(steps=[('pre', preprocessor_sklearn),\n",
    "                              ('gradientboostingclassifier', GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rs = RandomizedSearchCV(gb_pipeline, param_distributions=param_distributions,\n",
    "                        n_iter=20, scoring='roc_auc', cv=cv, verbose=2, n_jobs=-1, random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "print(\"Best CV AUC:\", rs.best_score_)\n",
    "\n",
    "best_gb = rs.best_estimator_\n",
    "proba_gb_best = best_gb.predict_proba(X_test)[:,1]\n",
    "print(\"Tuned Sklearn GB Test AUC:\", roc_auc_score(y_test, proba_gb_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280953f",
   "metadata": {},
   "source": [
    "### Feature importance (sklearn GB)\n",
    "\n",
    "`sklearn` exposes `feature_importances_` which is the (normalized) total reduction of the criterion brought by that feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after preprocessing\n",
    "feature_names_num = num_cols\n",
    "# for onehot, get names from the OneHotEncoder\n",
    "ohe = gb_pipeline.named_steps['pre'].named_transformers_['cat'].named_steps['onehot']\n",
    "ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "feature_names = feature_names_num + ohe_names\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = best_gb.named_steps['gradientboostingclassifier'].feature_importances_\n",
    "indices = np.argsort(importances)[-20:]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.title(\"Top 20 feature importances — sklearn GB\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742ad42",
   "metadata": {},
   "source": [
    "## Section 3 — XGBoost (`xgboost.XGBClassifier`)\n",
    "\n",
    "XGBoost uses a second-order Taylor expansion (gradient and hessian) for the loss at each boosting iteration. The per-step objective minimized is approximated as:\n",
    "\n",
    "$$\\mathcal{L}^{(t)} \\approx \\sum_i \\left[g_i f_t(x_i) + \\tfrac{1}{2} h_i f_t(x_i)^2\\right] + \\Omega(f_t)$$\n",
    "\n",
    "**Documentation:**\n",
    "- [XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)\n",
    "- [XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)\n",
    "\n",
    "**Main hyperparameters:** `n_estimators`, `learning_rate` (eta), `max_depth`, `min_child_weight`, `gamma`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`.\n",
    "\n",
    "1. **`n_estimators`**  \n",
    "   - The number of boosting rounds (trees) to fit.  \n",
    "   - More trees can improve performance but increase training time and risk overfitting.  \n",
    "   - Often tuned together with `learning_rate`.\n",
    "\n",
    "2. **`learning_rate` (`eta`)**  \n",
    "   - Step size shrinkage applied after each boosting step to prevent overfitting.  \n",
    "   - Lower values make the model more robust but require more trees (`n_estimators`).  \n",
    "   - Typical range: `0.01`–`0.3`.\n",
    "\n",
    "3. **`max_depth`**  \n",
    "   - Maximum depth of a tree.  \n",
    "   - Controls model complexity — deeper trees can capture more patterns but may overfit.  \n",
    "   - Typical range: `3`–`10`.\n",
    "\n",
    "4. **`min_child_weight`**  \n",
    "   - Minimum sum of instance weights (Hessian) in a child node.  \n",
    "   - Higher values make the algorithm more conservative (prevent overfitting).  \n",
    "   - Useful for controlling tree splitting when dataset has high variance.\n",
    "\n",
    "5. **`gamma`** (`min_split_loss`)  \n",
    "   - Minimum loss reduction required to make a further partition on a leaf node.  \n",
    "   - Larger values make the algorithm more conservative.  \n",
    "   - Acts as a regularization parameter for tree growth.\n",
    "\n",
    "6. **`subsample`**  \n",
    "   - Fraction of the training data randomly sampled for growing each tree.  \n",
    "   - Helps prevent overfitting.  \n",
    "   - Typical range: `0.5`–`1.0`.\n",
    "\n",
    "7. **`colsample_bytree`**  \n",
    "   - Fraction of features (columns) randomly sampled for each tree.  \n",
    "   - Helps reduce correlation between trees and overfitting.  \n",
    "   - Typical range: `0.5`–`1.0`.\n",
    "\n",
    "8. **`reg_alpha`** (L1 regularization term on weights)  \n",
    "   - Increases sparsity of weights (drives some leaf values to zero).  \n",
    "   - Can help in feature selection.\n",
    "\n",
    "9. **`reg_lambda`** (L2 regularization term on weights)  \n",
    "   - Penalizes large leaf weights.  \n",
    "   - Helps reduce model complexity and overfitting.\n",
    "\n",
    "xgboost all hyperparameters: https://xgboost.readthedocs.io/en/stable/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost (needs xgboost installed)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Prepare data with ordinal encoding (trees don't need one-hot)\n",
    "X_train_ord = ordinal_transformer.fit_transform(X_train)\n",
    "X_test_ord = ordinal_transformer.transform(X_test)\n",
    "\n",
    "xgb = XGBClassifier(eval_metric='auc', n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42)\n",
    "xgb.fit(X_train_ord, y_train)\n",
    "p_xgb = xgb.predict_proba(X_test_ord)[:,1]\n",
    "print(\"XGBoost Test AUC:\", roc_auc_score(y_test, p_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508a730",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (RandomizedSearchCV)\n",
    "\n",
    "This section runs `RandomizedSearchCV` for XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for XGBoost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 8),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0.5, 2)\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(eval_metric='auc', random_state=42)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rs_xgb = RandomizedSearchCV(xgb_clf, param_distributions=xgb_param_dist, n_iter=25, scoring='roc_auc', cv=cv, verbose=2, n_jobs=-1, random_state=42)\n",
    "rs_xgb.fit(X_train_ord, y_train)\n",
    "print(\"Best XGB params:\", rs_xgb.best_params_)\n",
    "print(\"Best XGB CV AUC:\", rs_xgb.best_score_)\n",
    "\n",
    "best_xgb = rs_xgb.best_estimator_\n",
    "p_xgb_best = best_xgb.predict_proba(X_test_ord)[:,1]\n",
    "print(\"Tuned XGBoost Test AUC:\", roc_auc_score(y_test, p_xgb_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7dea02",
   "metadata": {},
   "source": [
    "### Feature importance (XGBoost)\n",
    "\n",
    "XGBoost provides `feature_importances_` (based on gain, weight, cover) and `plot_importance` utilities. \n",
    "\n",
    "- **Weight (or Frequency):** This metric counts the number of times a feature is used to split the data across all trees in the model. Features with higher weight are used more frequently for splitting.\n",
    "\n",
    "- **Gain:** This is the most common and often preferred metric. It represents the average gain (reduction in impurity) achieved by splits involving a particular feature across all trees. A higher gain indicates a more significant contribution to reducing the model's error.\n",
    "\n",
    "- **Cover:** This metric reflects the average coverage or number of samples affected by splits involving a particular feature. It essentially measures the relative number of observations for which a feature is responsible in splits.\n",
    "\n",
    "\n",
    "We'll use `feature_importances_` (gain-based if `importance_type='gain'`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4445b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names used for XGBoost (ordinal mapping produced numeric array)\n",
    "# For ordinal_transformer, feature order is num_cols then cat_cols\n",
    "xgb_feature_names = num_cols + cat_cols\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = best_xgb.feature_importances_   # by default importance type is 'gain'\n",
    "indices = np.argsort(importances)[-20:]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [xgb_feature_names[i] for i in indices])\n",
    "plt.title(\"Top 20 feature importances — XGBoost (gain based)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = np.array(list(best_xgb.get_booster().get_score(importance_type='weight').values())) \n",
    "importances = importances / importances.sum()\n",
    "indices = np.argsort(importances)[-20:]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [xgb_feature_names[i] for i in indices])\n",
    "plt.title(\"Top 20 feature importances — XGBoost (weight based)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee97847",
   "metadata": {},
   "source": [
    "### Optional: Bayesian optimization with Optuna (XGBoost)\n",
    "\n",
    "This subsection uses **Optuna** to find a high-performing set of hyperparameters. It executes multiple trials — be prepared to wait depending on `n_trials`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna for XGBoost (ensure optuna is installed)\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0)\n",
    "    }\n",
    "    clf = XGBClassifier(use_label_encoder=False, eval_metric='auc', random_state=42, **param)\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X_train_ord, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "print(\"Best optuna params:\", study.best_params)\n",
    "# Train final with best params\n",
    "optuna_xgb = XGBClassifier(use_label_encoder=False, eval_metric='auc', random_state=42, **study.best_params)\n",
    "optuna_xgb.fit(X_train_ord, y_train)\n",
    "print(\"Optuna XGBoost Test AUC:\", roc_auc_score(y_test, optuna_xgb.predict_proba(X_test_ord)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d211c52",
   "metadata": {},
   "source": [
    "## Section 4 — LightGBM (`lightgbm.LGBMClassifier`)\n",
    "\n",
    "LightGBM is optimized for performance on large datasets using histogram-based algorithms and techniques like GOSS and EFB.\n",
    "\n",
    "**LightGBM Documentation:**\n",
    "- [LightGBM Classifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)\n",
    "- [LightGBM Regressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html)\n",
    "\n",
    "**Main hyperparameters:** `n_estimators`, `learning_rate`, `num_leaves`, `max_depth`, `min_data_in_leaf`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`.\n",
    "\n",
    "**LightGBM all hyperparameter:** https://lightgbm.readthedocs.io/en/latest/Parameters.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ab496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM (requires lightgbm installed)\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(n_estimators=500, learning_rate=0.05, random_state=42)\n",
    "# Use ordinal encoded arrays\n",
    "lgb_clf.fit(X_train_ord, y_train, eval_set=[(X_test_ord, y_test)], eval_metric='auc', early_stopping_rounds=50, verbose=False)\n",
    "print(\"LightGBM Test AUC:\", roc_auc_score(y_test, lgb_clf.predict_proba(X_test_ord)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf5f51",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (RandomizedSearchCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for LightGBM\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "lgb_param_dist = {\n",
    "    'n_estimators': randint(100, 800),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "rs_lgb = RandomizedSearchCV(lgb_model, param_distributions=lgb_param_dist, n_iter=25, scoring='roc_auc', cv=cv, verbose=2, n_jobs=-1, random_state=42)\n",
    "rs_lgb.fit(X_train_ord, y_train)\n",
    "print(\"Best LGB params:\", rs_lgb.best_params_)\n",
    "print(\"Best LGB CV AUC:\", rs_lgb.best_score_)\n",
    "\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "print(\"Tuned LightGBM Test AUC:\", roc_auc_score(y_test, best_lgb.predict_proba(X_test_ord)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecb1f1",
   "metadata": {},
   "source": [
    "### Feature importance (LightGBM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6601c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances from LightGBM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fi = best_lgb.feature_importances_\n",
    "indices = np.argsort(fi)[-20:]\n",
    "feature_names = num_cols + cat_cols\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(range(len(indices)), fi[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.title(\"Top 20 feature importances — LightGBM\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264de03",
   "metadata": {},
   "source": [
    "## Section 5 — CatBoost (`catboost.CatBoostClassifier`)\n",
    "\n",
    "CatBoost handles categorical features natively and uses ordered boosting to reduce target leakage. Provide categorical feature names/indices to the model.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "- [CatBoost Classifier](https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier)\n",
    "- [CatBoost Regressor](https://catboost.ai/docs/en/concepts/python-reference_catboostregressor)\n",
    "\n",
    "**Main hyperparameters:** `iterations`, `learning_rate`, `depth`, `l2_leaf_reg`, `bagging_temperature`, `border_count`.\n",
    "\n",
    "- `iterations`: Number of boosting rounds (trees) to build. More iterations can improve accuracy but risk overfitting.\n",
    "\n",
    "- `learning_rate`: Step size for updating trees. Smaller values improve generalization but need more iterations.\n",
    "\n",
    "- `depth`: Maximum depth of each tree, controlling model complexity. Higher depth captures interactions but risks overfitting.\n",
    "\n",
    "- `l2_leaf_reg`: L2 regularization coefficient for leaf values. Helps prevent overfitting by penalizing large weights.\n",
    "\n",
    "- `bagging_temperature`: Controls randomness in sampling. Higher values → more uniform sampling; lower values → greedier sampling of best points.\n",
    "\n",
    "- `border_count`: Number of splits (bins) used for numeric feature discretization. Larger values give finer splits but increase computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost (requires catboost installed)\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Prepare data: pass raw DataFrame and list of categorical feature names\n",
    "cat_features = cat_cols  # these are column NAMES in the original DataFrame\n",
    "cb = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=6, verbose=50, random_seed=42)\n",
    "cb.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "print(\"CatBoost Test AUC:\", roc_auc_score(y_test, cb.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945eb94",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (RandomizedSearchCV / CatBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for CatBoost (uses sklearn wrapper)\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "cb_param_dist = {\n",
    "    'iterations': randint(100, 800),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 8),\n",
    "    'l2_leaf_reg': uniform(1, 10)\n",
    "}\n",
    "\n",
    "cb_model = CatBoostClassifier(verbose=0, random_seed=42)\n",
    "rs_cb = RandomizedSearchCV(cb_model, param_distributions=cb_param_dist, n_iter=20, scoring='roc_auc', cv=cv, verbose=2, n_jobs=-1, random_state=42)\n",
    "# For CatBoost, we need to pass DataFrame and categorical feature indices to fit; RandomizedSearchCV will call fit with arrays.\n",
    "# To keep it simple, we'll fit on a converted dataset with ordinal encoding (already done above)\n",
    "rs_cb.fit(X_train_ord, y_train)\n",
    "print(\"Best CatBoost params:\", rs_cb.best_params_)\n",
    "print(\"Best CatBoost CV AUC:\", rs_cb.best_score_)\n",
    "\n",
    "best_cb = rs_cb.best_estimator_\n",
    "print(\"Tuned CatBoost Test AUC:\", roc_auc_score(y_test, best_cb.predict_proba(X_test_ord)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68bef3",
   "metadata": {},
   "source": [
    "### Feature importance (CatBoost)\n",
    "\n",
    "CatBoost exposes `get_feature_importance` which supports several importance types (PredictionValuesChange, LossFunctionChange, ShapValues, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost feature importance (using PredictionValuesChange)\n",
    "fi_cb = cb.get_feature_importance(type='PredictionValuesChange', data=cb.get_pool(X_train, label=y_train, cat_features=cat_features))\n",
    "feature_names_all = X_train.columns.tolist()\n",
    "indices = np.argsort(fi_cb)[-20:]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(range(len(indices)), fi_cb[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names_all[i] for i in indices])\n",
    "plt.title(\"Top 20 feature importances — CatBoost (PredictionValuesChange)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de944ad",
   "metadata": {},
   "source": [
    "## Summary & Practical tips\n",
    "\n",
    "- All four gradient-boosting families are included with tuning and feature importance.\n",
    "- Running all tuning steps will take time; adjust `n_iter` / `n_trials` for faster runs.\n",
    "- Use early stopping where supported to reduce overfitting and speed up tuning.\n",
    "\n",
    "---\n",
    "\n",
    "Place `bank-additional-full.csv` in the same folder as this notebook and run all cells.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
