{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f31365",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f822915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12576d0e",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier from scratch in python\n",
    "\n",
    "We will implement ensemble AdaBoost Classifier with Decision Stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e625dbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1,  1, -1,  1, -1,  1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(['yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes'])\n",
    "\n",
    "classes_ = np.unique(y)\n",
    "\n",
    "classes_[0], classes_[1]\n",
    "\n",
    "y_encoded = np.where(y == classes_[0], -1, 1)\n",
    "\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a04349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = len(y)\n",
    "\n",
    "np.ones(n_samples)/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabe651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.epsilon = 1e-8\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Convert labels to {-1, +1} format for AdaBoost\n",
    "        self.classes_ = np.unique(y)\n",
    "        y_encoded = np.where(y == self.classes_[0], -1, 1)\n",
    "        \n",
    "        # Initialize weights uniformly\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for t in range(self.n_estimators):\n",
    "            # Train weak learner (decision stump), we can use other type of weak learner as well\n",
    "            model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "            model.fit(X, y, sample_weight=w)  # we are using weighted gini\n",
    "            \n",
    "            # Get predictions and convert to {-1, +1}\n",
    "            predictions = model.predict(X)\n",
    "            predictions_encoded = np.where(predictions == self.classes_[0], -1, 1)\n",
    "            \n",
    "            # Calculate weighted error\n",
    "            error = np.sum(w * (predictions_encoded != y_encoded)) / np.sum(w)\n",
    "            \n",
    "            # Handle edge cases for error\n",
    "            if error >= 0.5:\n",
    "                # If error >= 0.5, weak learner is no better than random\n",
    "                if len(self.models) == 0:\n",
    "                    # First iteration, continue with small adjustment\n",
    "                    error = 0.5 - self.epsilon\n",
    "                else:\n",
    "                    # Stop adding more weak learners\n",
    "                    break\n",
    "            \n",
    "            if error <= 0:\n",
    "                error = self.epsilon\n",
    "            \n",
    "            # Calculate alpha (weak learner influence)\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            # Store model and alpha\n",
    "            self.models.append(model)\n",
    "            self.alphas.append(alpha)\n",
    "            \n",
    "            # Update sample weights\n",
    "            w = w * np.exp(-alpha * y_encoded * predictions_encoded)\n",
    "            w = w / np.sum(w)  # Normalize weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.models:\n",
    "            raise ValueError(\"Model has not been fitted yet.\")\n",
    "        \n",
    "        # Initialize predictions\n",
    "        final_predictions = np.zeros(X.shape[0])\n",
    "        \n",
    "        # Combine predictions from all weak learners\n",
    "        for model, alpha in zip(self.models, self.alphas):\n",
    "            predictions = model.predict(X)\n",
    "            predictions_encoded = np.where(predictions == self.classes_[0], -1, 1)\n",
    "            final_predictions += alpha * predictions_encoded\n",
    "        \n",
    "        # Convert back to original label format\n",
    "        binary_predictions = np.where(final_predictions >= 0, 1, -1)\n",
    "        return np.where(binary_predictions == -1, self.classes_[0], self.classes_[1])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\"Model has not been fitted yet.\")\n",
    "        \n",
    "        final_predictions = np.zeros(X.shape[0])\n",
    "        \n",
    "        for model, alpha in zip(self.models, self.alphas):\n",
    "            predictions = model.predict(X)\n",
    "            predictions_encoded = np.where(predictions == self.classes_[0], -1, 1)\n",
    "            final_predictions += alpha * predictions_encoded\n",
    "        \n",
    "        # Convert to probabilities using sigmoid-like transformation\n",
    "        prob_class_1 = 1 / (1 + np.exp(-2 * final_predictions))\n",
    "        prob_class_0 = 1 - prob_class_1\n",
    "        \n",
    "        return np.column_stack((prob_class_0, prob_class_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438963f9",
   "metadata": {},
   "source": [
    "### Create synthtetic dataset and measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4926f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.3333%\n",
      "Precision: 0.8194\n",
      "Recall: 0.8881\n",
      "F1 Score: 0.8523\n"
     ]
    }
   ],
   "source": [
    "# Create a synthtic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Create a ensemble Adaboost classifier\n",
    "adaboost = AdaBoost(n_estimators=50)\n",
    "\n",
    "# Fitting the classifier\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test dataset\n",
    "predictions = adaboost.predict(X_test)\n",
    "\n",
    "# Measure the performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82d75f",
   "metadata": {},
   "source": [
    "### Sklearn in-built AdaBoost Classifier\n",
    "\n",
    "sklearn adaboost classifier documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "sklearn adaboost regressor documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591ff2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab69ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.3333%\n",
      "Precision: 0.8194\n",
      "Recall: 0.8881\n",
      "F1 Score: 0.8523\n"
     ]
    }
   ],
   "source": [
    "# define the model. The estimator parameter is optional. By default it will take a decision stump\n",
    "adaboost_sklearn = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                                      n_estimators=50,\n",
    "                                      random_state=42)\n",
    "\n",
    "# fitting the model\n",
    "adaboost_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test dataset\n",
    "predictions = adaboost_sklearn.predict(X_test)\n",
    "\n",
    "# Measure the performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ef748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
