{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7226ed58",
   "metadata": {},
   "source": [
    "# Text Preprocessing Basics\n",
    "\n",
    "In this notebook we shall discuss different preprocessing steps to apply on tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2a53f",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "We will use **nltk**. If nltk is not installed, you can install it by `pip install nltk` in your virtual environment. Once nltk is installed do the following:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')          # this will download the stopwords\n",
    "nltk.download('twitter_samples')    # this will download labelled twitter samples for sentiment analysis\n",
    "nltk.download('wordnet')            # this will download the wordnet for lemmatization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85db7b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sourav\n",
      "[nltk_data]     Karmakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Sourav\n",
      "[nltk_data]     Karmakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sourav\n",
      "[nltk_data]     Karmakar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96badfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:14:13.058357Z",
     "start_time": "2022-11-27T13:14:10.219712Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c51db03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812f936",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4292c",
   "metadata": {},
   "source": [
    "The `twitter_samples` contains subsets of 5000 positive tweets and 5000 negative tweets, and a full set of 10000 tweets.\n",
    "We need to use 5000 positive and 5000 negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cba9a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:17:07.097857Z",
     "start_time": "2022-11-27T13:17:03.729672Z"
    }
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650bc7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:18:49.435143Z",
     "start_time": "2022-11-27T13:18:49.423021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6942c929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:18:58.575210Z",
     "start_time": "2022-11-27T13:18:58.565705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(',\n",
       " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
       " 'Dang starting next week I have \"work\" :(']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_negative_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10751a00",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af14643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:43:15.766988Z",
     "start_time": "2022-11-27T13:43:15.760169Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604b56ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:30:41.462112Z",
     "start_time": "2022-11-27T13:30:41.442129Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ffe1eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:30:58.311464Z",
     "start_time": "2022-11-27T13:30:58.302418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812d8d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_english[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7248e040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:33:00.940795Z",
     "start_time": "2022-11-27T13:33:00.921353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = all_positive_tweets[0]\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd2b480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:33:22.848210Z",
     "start_time": "2022-11-27T13:33:22.840242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FollowFriday France_Inte PKuchly57 Milipol_Paris for being top engaged members in my community this week :)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'@', '', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977b503",
   "metadata": {},
   "source": [
    "### Difference between Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b10304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words:  ['studi', 'studi', 'chang', 'care', 'fair']\n",
      "Lemmatized words:  ['study', 'study', 'change', 'care', 'fairness']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"studies\", \"studying\", \"changing\", \"caring\", \"fairness\"]\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "print(\"Stemmed words: \", stemmed_words)\n",
    "\n",
    "print(\"Lemmatized words: \", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0f15710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:50:28.683424Z",
     "start_time": "2022-11-27T13:50:28.668124Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "    This function process a tweet\n",
    "    input:\n",
    "        tweet: string -> a string containing tweet\n",
    "    output:\n",
    "        tweet_clean: string -> a list of words containing processed tweets\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # remove the hashtags -> remove the # sign from the word\n",
    "    tweet = re.sub(r'#','',tweet)\n",
    "    \n",
    "    # remove the stock market tickers like $AMAZON\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+','', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove numbers\n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
    "\n",
    "    # remove simple emoticons like :) :( :D :/ :-/ :-(\n",
    "    tweet = re.sub(r'[:;=8][-]?[)(/DPp]', '', tweet)\n",
    "    \n",
    "    # tokenize the tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and   # removes stopwords\n",
    "            word not in string.punctuation):    # removes punctuations\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "            \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "511279c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5077949b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:50:29.865029Z",
     "start_time": "2022-11-27T13:50:29.839899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week']\n",
      "['hey', 'jame', 'odd', 'pleas', 'call', 'contact', 'centr', 'abl', 'assist', 'mani', 'thank']\n",
      "['listen', 'last', 'night', 'bleed', 'amaz', 'track', 'scotland']\n",
      "['congrat']\n",
      "['yeaaah', 'yipppi', 'accnt', 'verifi', 'rqst', 'succeed', 'got', 'blue', 'tick', 'mark', 'fb', 'profil', 'day']\n"
     ]
    }
   ],
   "source": [
    "for x in all_positive_tweets[:5]:\n",
    "    print(process_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9510487a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(',\n",
       " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
       " 'Dang starting next week I have \"work\" :(']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_negative_tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea1f25e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T13:51:23.309116Z",
     "start_time": "2022-11-27T13:51:23.291353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hopeless', 'tmr']\n",
      "['everyth', 'kid', 'section', 'ikea', 'cute', 'shame', 'nearli', 'month']\n",
      "['heart', 'slide', 'wast', 'basket']\n",
      "['“', 'hate', 'japanes', 'call', 'bani', '”']\n",
      "['dang', 'start', 'next', 'week', 'work']\n"
     ]
    }
   ],
   "source": [
    "for x in all_negative_tweets[:5]:\n",
    "    print(process_tweet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14bf02",
   "metadata": {},
   "source": [
    "## Generate vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd40f586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:21:11.630240Z",
     "start_time": "2022-11-27T14:21:11.620417Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"I love my country :)\", \"I hate EDM\", \"I love to do math\", \"I am very bad at history :(\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17eb66ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:20:58.010012Z",
     "start_time": "2022-11-27T14:20:58.002195Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(corpus):\n",
    "    vocabulary = []\n",
    "    for x in corpus:\n",
    "        for word in process_tweet(x):\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dce62ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:21:26.399505Z",
     "start_time": "2022-11-27T14:21:26.376185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'countri', 'hate', 'edm', 'math', 'bad', 'histori']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = create_vocabulary(corpus)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6eda4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:21:28.157138Z",
     "start_time": "2022-11-27T14:21:28.147062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6855c28",
   "metadata": {},
   "source": [
    "Let's create the vocabulary for our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c10987fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:22:19.682617Z",
     "start_time": "2022-11-27T14:22:19.666150Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_corpus = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9be49b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:22:27.079657Z",
     "start_time": "2022-11-27T14:22:27.060485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89414410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:22:39.377047Z",
     "start_time": "2022-11-27T14:22:32.313767Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_vocab = create_vocabulary(tweet_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce9f04d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T14:22:44.983194Z",
     "start_time": "2022-11-27T14:22:44.963070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9870"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a712ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['followfriday',\n",
       " 'top',\n",
       " 'engag',\n",
       " 'member',\n",
       " 'commun',\n",
       " 'week',\n",
       " 'hey',\n",
       " 'jame',\n",
       " 'odd',\n",
       " 'pleas',\n",
       " 'call',\n",
       " 'contact',\n",
       " 'centr',\n",
       " 'abl',\n",
       " 'assist',\n",
       " 'mani',\n",
       " 'thank',\n",
       " 'listen',\n",
       " 'last',\n",
       " 'night',\n",
       " 'bleed',\n",
       " 'amaz',\n",
       " 'track',\n",
       " 'scotland',\n",
       " 'congrat',\n",
       " 'yeaaah',\n",
       " 'yipppi',\n",
       " 'accnt',\n",
       " 'verifi',\n",
       " 'rqst',\n",
       " 'succeed',\n",
       " 'got',\n",
       " 'blue',\n",
       " 'tick',\n",
       " 'mark',\n",
       " 'fb',\n",
       " 'profil',\n",
       " 'day',\n",
       " 'one',\n",
       " 'irresist',\n",
       " 'flipkartfashionfriday',\n",
       " 'like',\n",
       " 'keep',\n",
       " 'love',\n",
       " 'custom',\n",
       " 'wait',\n",
       " 'long',\n",
       " 'hope',\n",
       " 'enjoy',\n",
       " 'happi',\n",
       " 'friday',\n",
       " 'lwwf',\n",
       " 'second',\n",
       " 'thought',\n",
       " '’',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'dd',\n",
       " 'new',\n",
       " 'short',\n",
       " 'enter',\n",
       " 'system',\n",
       " 'sheep',\n",
       " 'must',\n",
       " 'buy',\n",
       " 'jgh',\n",
       " 'go',\n",
       " 'bayan',\n",
       " 'bye',\n",
       " 'act',\n",
       " 'mischiev',\n",
       " 'etl',\n",
       " 'layer',\n",
       " 'in-hous',\n",
       " 'wareh',\n",
       " 'app',\n",
       " 'katamari',\n",
       " 'well',\n",
       " '…',\n",
       " 'name',\n",
       " 'impli',\n",
       " 'influenc',\n",
       " 'big',\n",
       " '...',\n",
       " 'juici',\n",
       " 'selfi',\n",
       " 'follow',\n",
       " 'perfect',\n",
       " 'alreadi',\n",
       " 'know',\n",
       " \"what'\",\n",
       " 'great',\n",
       " 'opportun',\n",
       " 'junior',\n",
       " 'triathlet',\n",
       " 'age',\n",
       " 'gatorad',\n",
       " 'seri',\n",
       " 'get',\n",
       " 'entri',\n",
       " 'lay',\n",
       " 'greet',\n",
       " 'card',\n",
       " 'rang',\n",
       " 'print',\n",
       " 'today',\n",
       " 'job',\n",
       " \"friend'\",\n",
       " 'lunch',\n",
       " 'yummm',\n",
       " 'nostalgia',\n",
       " 'tb',\n",
       " 'ku',\n",
       " 'id',\n",
       " 'conflict',\n",
       " 'help',\n",
       " \"here'\",\n",
       " 'screenshot',\n",
       " 'work',\n",
       " 'hi',\n",
       " 'liv',\n",
       " 'hello',\n",
       " 'need',\n",
       " 'someth',\n",
       " 'u',\n",
       " 'fm',\n",
       " 'twitter',\n",
       " '—',\n",
       " 'sure',\n",
       " 'thing',\n",
       " 'dm',\n",
       " 'x',\n",
       " 'heard',\n",
       " 'four',\n",
       " 'season',\n",
       " 'pretti',\n",
       " 'dope',\n",
       " 'penthous',\n",
       " 'obv',\n",
       " 'gobigorgohom',\n",
       " 'fun',\n",
       " \"y'all\",\n",
       " 'yeah',\n",
       " 'suppos',\n",
       " 'lol',\n",
       " 'chat',\n",
       " 'bit',\n",
       " 'youth',\n",
       " '💅🏽',\n",
       " '💋',\n",
       " 'seen',\n",
       " 'year',\n",
       " 'rest',\n",
       " 'goe',\n",
       " 'quickli',\n",
       " 'bed',\n",
       " 'music',\n",
       " 'fix',\n",
       " 'dream',\n",
       " 'spiritu',\n",
       " 'ritual',\n",
       " 'festiv',\n",
       " 'népal',\n",
       " 'begin',\n",
       " 'line-up',\n",
       " 'left',\n",
       " 'see',\n",
       " 'sarah',\n",
       " 'send',\n",
       " 'us',\n",
       " 'email',\n",
       " 'bitsy@bitdefender.com',\n",
       " 'asap',\n",
       " 'kik',\n",
       " 'hatessuc',\n",
       " 'kikm',\n",
       " 'lgbt',\n",
       " 'tinder',\n",
       " 'nsfw',\n",
       " 'akua',\n",
       " 'cumshot',\n",
       " 'come',\n",
       " 'hous',\n",
       " 'nsn_supplement',\n",
       " 'effect',\n",
       " 'press',\n",
       " 'releas',\n",
       " 'distribut',\n",
       " 'result',\n",
       " 'link',\n",
       " 'remov',\n",
       " 'pressreleas',\n",
       " 'newsdistribut',\n",
       " 'bam',\n",
       " 'bestfriend',\n",
       " 'lot',\n",
       " 'warsaw',\n",
       " 'everyon',\n",
       " 'watch',\n",
       " 'documentari',\n",
       " 'earthl',\n",
       " 'youtub',\n",
       " 'support',\n",
       " 'buuut',\n",
       " 'oh',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'visit',\n",
       " 'next',\n",
       " 'letsgetmessi',\n",
       " 'jo',\n",
       " 'make',\n",
       " 'feel',\n",
       " 'better',\n",
       " 'never',\n",
       " 'anyon',\n",
       " 'kpop',\n",
       " 'flesh',\n",
       " 'good',\n",
       " 'girl',\n",
       " 'best',\n",
       " 'wish',\n",
       " 'reason',\n",
       " 'epic',\n",
       " 'soundtrack',\n",
       " 'shout',\n",
       " 'ad',\n",
       " 'video',\n",
       " 'playlist',\n",
       " 'would',\n",
       " 'dear',\n",
       " 'jordan',\n",
       " 'okay',\n",
       " 'fake',\n",
       " 'gameplay',\n",
       " 'haha',\n",
       " 'im',\n",
       " 'kid',\n",
       " 'stuff',\n",
       " 'exactli',\n",
       " 'product',\n",
       " 'line',\n",
       " 'etsi',\n",
       " 'shop',\n",
       " 'check',\n",
       " 'vacat',\n",
       " 'recharg',\n",
       " 'normal',\n",
       " 'charger',\n",
       " 'asleep',\n",
       " 'talk',\n",
       " 'sooo',\n",
       " 'someon',\n",
       " 'text',\n",
       " 'ye',\n",
       " 'bet',\n",
       " 'fit',\n",
       " 'hear',\n",
       " 'speech',\n",
       " 'piti',\n",
       " 'green',\n",
       " 'garden',\n",
       " 'midnight',\n",
       " 'sun',\n",
       " 'beauti',\n",
       " 'canal',\n",
       " 'dasvidaniya',\n",
       " 'till',\n",
       " 'scout',\n",
       " 'sg',\n",
       " 'futur',\n",
       " 'wlan',\n",
       " 'pro',\n",
       " 'confer',\n",
       " 'asia',\n",
       " 'chang',\n",
       " 'lollipop',\n",
       " '🍭',\n",
       " 'nez',\n",
       " 'agnezmo',\n",
       " 'oley',\n",
       " 'mama',\n",
       " 'stand',\n",
       " 'stronger',\n",
       " 'god',\n",
       " 'misti',\n",
       " 'babi',\n",
       " 'cute',\n",
       " 'woohoo',\n",
       " \"can't\",\n",
       " 'sign',\n",
       " 'yet',\n",
       " 'still',\n",
       " 'think',\n",
       " 'mka',\n",
       " 'liam',\n",
       " 'access',\n",
       " 'welcom',\n",
       " 'stat',\n",
       " 'arriv',\n",
       " 'unfollow',\n",
       " 'via',\n",
       " 'surpris',\n",
       " 'figur',\n",
       " 'happybirthdayemilybett',\n",
       " 'sweet',\n",
       " 'talent',\n",
       " 'plan',\n",
       " 'drain',\n",
       " 'gotta',\n",
       " 'timezon',\n",
       " 'parent',\n",
       " 'proud',\n",
       " 'least',\n",
       " 'mayb',\n",
       " 'sometim',\n",
       " 'grade',\n",
       " 'al',\n",
       " 'grand',\n",
       " 'manila_bro',\n",
       " 'chosen',\n",
       " 'let',\n",
       " 'around',\n",
       " '..',\n",
       " 'side',\n",
       " 'world',\n",
       " 'eh',\n",
       " 'take',\n",
       " 'care',\n",
       " 'final',\n",
       " 'fuck',\n",
       " 'weekend',\n",
       " 'real',\n",
       " 'join',\n",
       " 'hushedcallwithfraydo',\n",
       " 'gift',\n",
       " 'yeahhh',\n",
       " 'hushedpinwithsammi',\n",
       " 'event',\n",
       " 'might',\n",
       " 'luv',\n",
       " 'realli',\n",
       " 'appreci',\n",
       " 'share',\n",
       " 'wow',\n",
       " 'tom',\n",
       " 'gym',\n",
       " 'monday',\n",
       " 'invit',\n",
       " 'scope',\n",
       " 'friend',\n",
       " 'nude',\n",
       " 'sleep',\n",
       " 'birthday',\n",
       " 'want',\n",
       " 't-shirt',\n",
       " 'cool',\n",
       " 'haw',\n",
       " 'phela',\n",
       " 'mom',\n",
       " 'obvious',\n",
       " 'princ',\n",
       " 'charm',\n",
       " 'stage',\n",
       " 'luck',\n",
       " 'tyler',\n",
       " 'hipster',\n",
       " 'glass',\n",
       " 'marti',\n",
       " 'glad',\n",
       " 'done',\n",
       " 'afternoon',\n",
       " 'read',\n",
       " 'kahfi',\n",
       " 'finish',\n",
       " 'ohmyg',\n",
       " 'yaya',\n",
       " 'dub',\n",
       " 'stalk',\n",
       " 'ig',\n",
       " 'gondooo',\n",
       " 'moo',\n",
       " 'tologooo',\n",
       " 'becom',\n",
       " 'detail',\n",
       " 'zzz',\n",
       " 'xx',\n",
       " 'physiotherapi',\n",
       " 'hashtag',\n",
       " '💪',\n",
       " 'monica',\n",
       " 'miss',\n",
       " 'sound',\n",
       " 'morn',\n",
       " \"that'\",\n",
       " 'definit',\n",
       " 'tri',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'advic',\n",
       " 'treviso',\n",
       " 'concert',\n",
       " 'citi',\n",
       " 'countri',\n",
       " 'start',\n",
       " 'fine',\n",
       " 'gorgeou',\n",
       " 'xo',\n",
       " 'oven',\n",
       " 'roast',\n",
       " 'garlic',\n",
       " 'oliv',\n",
       " 'oil',\n",
       " 'dri',\n",
       " 'tomato',\n",
       " 'basil',\n",
       " 'centuri',\n",
       " 'tuna',\n",
       " 'right',\n",
       " 'back',\n",
       " 'atchya',\n",
       " 'even',\n",
       " 'almost',\n",
       " 'chanc',\n",
       " 'cheer',\n",
       " 'po',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'agre',\n",
       " 'heheheh',\n",
       " 'that',\n",
       " 'point',\n",
       " 'stay',\n",
       " 'home',\n",
       " 'soon',\n",
       " 'promis',\n",
       " 'web',\n",
       " 'whatsapp',\n",
       " 'volta',\n",
       " 'funcionar',\n",
       " 'com',\n",
       " 'iphon',\n",
       " 'jailbroken',\n",
       " 'later',\n",
       " 'min',\n",
       " 'leia',\n",
       " 'appear',\n",
       " 'hologram',\n",
       " 'rd',\n",
       " 'w',\n",
       " 'messag',\n",
       " 'obi',\n",
       " 'wan',\n",
       " 'sit',\n",
       " 'luke',\n",
       " 'inter',\n",
       " 'ucl',\n",
       " 'arsen',\n",
       " 'small',\n",
       " 'team',\n",
       " 'pass',\n",
       " '🚂',\n",
       " 'dewsburi',\n",
       " 'railway',\n",
       " 'station',\n",
       " 'dew',\n",
       " 'west',\n",
       " 'yorkshir',\n",
       " 'smh',\n",
       " 'live',\n",
       " 'strang',\n",
       " 'imagin',\n",
       " 'megan',\n",
       " 'masaantoday',\n",
       " 'shweta',\n",
       " 'tripathi',\n",
       " '. .',\n",
       " 'kurta',\n",
       " 'half',\n",
       " 'number',\n",
       " 'wsalelov',\n",
       " 'ah',\n",
       " 'larri',\n",
       " 'anyway',\n",
       " 'kinda',\n",
       " 'goood',\n",
       " 'life',\n",
       " 'enn',\n",
       " 'could',\n",
       " 'warmup',\n",
       " 'th',\n",
       " 'bath',\n",
       " 'dum',\n",
       " 'andar',\n",
       " 'ram',\n",
       " 'sampath',\n",
       " 'sona',\n",
       " 'mohapatra',\n",
       " 'samantha',\n",
       " 'edward',\n",
       " 'mein',\n",
       " 'tulan',\n",
       " 'razi',\n",
       " 'wah',\n",
       " 'josh',\n",
       " 'alway',\n",
       " 'smile',\n",
       " 'pictur',\n",
       " 'giveitup',\n",
       " 'given',\n",
       " 'ga',\n",
       " 'subsidi',\n",
       " 'initi',\n",
       " 'propos',\n",
       " 'delight',\n",
       " 'yesterday',\n",
       " 'lmaoo',\n",
       " 'song',\n",
       " 'ever',\n",
       " 'shall',\n",
       " 'littl',\n",
       " 'throwback',\n",
       " 'outli',\n",
       " 'island',\n",
       " 'cheung',\n",
       " 'chau',\n",
       " 'mui',\n",
       " 'wo',\n",
       " 'total',\n",
       " 'differ',\n",
       " 'kfckitchentour',\n",
       " 'kitchen',\n",
       " 'clean',\n",
       " 'cusp',\n",
       " 'test',\n",
       " 'water',\n",
       " 'reward',\n",
       " 'arummzz',\n",
       " \"let'\",\n",
       " 'drive',\n",
       " 'travel',\n",
       " 'yogyakarta',\n",
       " 'jeep',\n",
       " 'indonesia',\n",
       " 'instamood',\n",
       " 'wanna',\n",
       " 'skype',\n",
       " 'may',\n",
       " 'nice',\n",
       " 'friendli',\n",
       " 'pretend',\n",
       " 'film',\n",
       " 'congratul',\n",
       " 'winner',\n",
       " 'cheesydelight',\n",
       " 'contest',\n",
       " 'address',\n",
       " 'guy',\n",
       " 'market',\n",
       " 'hour',\n",
       " 'leav',\n",
       " 'without',\n",
       " 'delay',\n",
       " 'actual',\n",
       " 'easi',\n",
       " 'guess',\n",
       " 'train',\n",
       " 'wd',\n",
       " 'shift',\n",
       " 'engin',\n",
       " 'etc',\n",
       " 'sunburn',\n",
       " 'peel',\n",
       " 'blog',\n",
       " 'huge',\n",
       " 'warm',\n",
       " '☆',\n",
       " 'complet',\n",
       " 'triangl',\n",
       " 'northern',\n",
       " 'ireland',\n",
       " 'sight',\n",
       " 'smthng',\n",
       " 'fr',\n",
       " 'hug',\n",
       " 'xoxo',\n",
       " 'uu',\n",
       " 'jaann',\n",
       " 'topnewfollow',\n",
       " 'connect',\n",
       " 'wonder',\n",
       " 'made',\n",
       " 'fluffi',\n",
       " 'insid',\n",
       " 'pirouett',\n",
       " 'moos',\n",
       " 'trip',\n",
       " 'philli',\n",
       " 'decemb',\n",
       " 'dude',\n",
       " 'question',\n",
       " 'flaw',\n",
       " 'pain',\n",
       " 'negat',\n",
       " 'strength',\n",
       " 'went',\n",
       " 'solo',\n",
       " 'move',\n",
       " 'fav',\n",
       " 'nirvana',\n",
       " 'smell',\n",
       " 'teen',\n",
       " 'spirit',\n",
       " 'rip',\n",
       " 'ami',\n",
       " 'winehous',\n",
       " 'coupl',\n",
       " 'tomhiddleston',\n",
       " 'elizabetholsen',\n",
       " 'yaytheylookgreat',\n",
       " 'goodnight',\n",
       " 'vid',\n",
       " 'wake',\n",
       " 'gonna',\n",
       " 'shoot',\n",
       " 'itti',\n",
       " 'bitti',\n",
       " 'teeni',\n",
       " 'bikini',\n",
       " 'much',\n",
       " 'togeth',\n",
       " 'end',\n",
       " 'xfile',\n",
       " 'content',\n",
       " 'rain',\n",
       " 'fabul',\n",
       " 'fantast',\n",
       " '♡',\n",
       " 'jb',\n",
       " 'forev',\n",
       " 'belieb',\n",
       " 'nighti',\n",
       " 'bug',\n",
       " 'bite',\n",
       " 'bracelet',\n",
       " 'idea',\n",
       " 'foundri',\n",
       " 'game',\n",
       " 'sens',\n",
       " 'pic',\n",
       " 'ef',\n",
       " 'phone',\n",
       " 'woot',\n",
       " 'derek',\n",
       " 'use',\n",
       " 'parkshar',\n",
       " 'gloucestershir',\n",
       " 'aaaahhh',\n",
       " 'man',\n",
       " 'traffic',\n",
       " 'stress',\n",
       " 'reliev',\n",
       " \"how'r\",\n",
       " 'arbeloa',\n",
       " 'turn',\n",
       " 'omg',\n",
       " 'say',\n",
       " 'europ',\n",
       " 'rise',\n",
       " 'find',\n",
       " 'hard',\n",
       " 'believ',\n",
       " 'uncount',\n",
       " 'coz',\n",
       " 'unlimit',\n",
       " 'cours',\n",
       " 'teamposit',\n",
       " 'aldub',\n",
       " '☕',\n",
       " 'rita',\n",
       " 'info',\n",
       " 'way',\n",
       " 'boy',\n",
       " 'true',\n",
       " 'sethi',\n",
       " 'high',\n",
       " 'exe',\n",
       " 'skeem',\n",
       " 'saam',\n",
       " 'peopl',\n",
       " 'polit',\n",
       " 'izzat',\n",
       " 'wese',\n",
       " 'trust',\n",
       " 'khawateen',\n",
       " 'k',\n",
       " 'sath',\n",
       " 'mana',\n",
       " 'kar',\n",
       " 'deya',\n",
       " 'sort',\n",
       " 'smart',\n",
       " 'hair',\n",
       " 'tbh',\n",
       " 'jacob',\n",
       " 'g',\n",
       " 'upgrad',\n",
       " 'tee',\n",
       " 'famili',\n",
       " 'person',\n",
       " 'two',\n",
       " 'convers',\n",
       " 'onlin',\n",
       " 'mclaren',\n",
       " 'fridayfeel',\n",
       " 'tgif',\n",
       " 'squar',\n",
       " 'enix',\n",
       " 'bissmillah',\n",
       " 'ya',\n",
       " 'allah',\n",
       " 'socent',\n",
       " 'startup',\n",
       " 'drop',\n",
       " 'your',\n",
       " 'arnd',\n",
       " 'town',\n",
       " 'basic',\n",
       " 'piss',\n",
       " 'cup',\n",
       " 'also',\n",
       " 'terribl',\n",
       " 'complic',\n",
       " 'discuss',\n",
       " 'snapchat',\n",
       " 'lynettelow',\n",
       " 'kikmenow',\n",
       " 'snapm',\n",
       " 'hot',\n",
       " 'amazon',\n",
       " 'kikmeguy',\n",
       " 'defin',\n",
       " 'grow',\n",
       " 'sport',\n",
       " 'rt',\n",
       " 'rakyat',\n",
       " 'write',\n",
       " 'sinc',\n",
       " 'mention',\n",
       " 'fli',\n",
       " 'fish',\n",
       " 'promot',\n",
       " 'post',\n",
       " 'cyber',\n",
       " 'ourdaughtersourprid',\n",
       " 'mypapamyprid',\n",
       " 'papa',\n",
       " 'coach',\n",
       " 'posit',\n",
       " 'kha',\n",
       " 'atleast',\n",
       " 'mango',\n",
       " \"lassi'\",\n",
       " \"monty'\",\n",
       " 'marvel',\n",
       " 'though',\n",
       " 'suspect',\n",
       " 'meant',\n",
       " 'hr',\n",
       " 'touch',\n",
       " 'kepler',\n",
       " 'b',\n",
       " 'chalna',\n",
       " 'hai',\n",
       " 'thankyou',\n",
       " 'hazel',\n",
       " 'food',\n",
       " 'brooklyn',\n",
       " 'pta',\n",
       " 'awak',\n",
       " 'okayi',\n",
       " 'awww',\n",
       " 'ha',\n",
       " 'doc',\n",
       " 'splendid',\n",
       " 'spam',\n",
       " 'folder',\n",
       " 'amount',\n",
       " 'nigeria',\n",
       " 'claim',\n",
       " 'rted',\n",
       " 'leg',\n",
       " 'hurt',\n",
       " 'bad',\n",
       " 'mine',\n",
       " 'saturday',\n",
       " 'thaaank',\n",
       " 'puhon',\n",
       " 'happinesss',\n",
       " 'tnc',\n",
       " 'prior',\n",
       " 'notif',\n",
       " 'fat',\n",
       " 'co',\n",
       " 'probabl',\n",
       " 'ate',\n",
       " 'yuna',\n",
       " 'tamesid',\n",
       " '´',\n",
       " 'googl',\n",
       " 'account',\n",
       " 'scouser',\n",
       " 'everyth',\n",
       " 'zoe',\n",
       " 'mate',\n",
       " 'liter',\n",
       " 'samee',\n",
       " 'edgar',\n",
       " 'updat',\n",
       " 'log',\n",
       " 'bring',\n",
       " 'abe',\n",
       " 'meet',\n",
       " 'sigh',\n",
       " 'dreamili',\n",
       " 'pout',\n",
       " 'eye',\n",
       " 'quacketyquack',\n",
       " 'funni',\n",
       " 'happen',\n",
       " 'phil',\n",
       " 'em',\n",
       " 'del',\n",
       " 'rodder',\n",
       " 'els',\n",
       " 'play',\n",
       " 'newest',\n",
       " 'gamejam',\n",
       " 'irish',\n",
       " 'literatur',\n",
       " 'inaccess',\n",
       " \"kareena'\",\n",
       " 'fan',\n",
       " 'brain',\n",
       " 'dot',\n",
       " 'braindot',\n",
       " 'fair',\n",
       " 'rush',\n",
       " 'either',\n",
       " 'brandi',\n",
       " 'carniv',\n",
       " 'men',\n",
       " 'put',\n",
       " 'mask',\n",
       " 'xavier',\n",
       " 'forneret',\n",
       " 'jennif',\n",
       " 'site',\n",
       " 'free',\n",
       " 'ball',\n",
       " 'pool',\n",
       " 'coin',\n",
       " 'edit',\n",
       " 'trish',\n",
       " '♥',\n",
       " 'grate',\n",
       " 'three',\n",
       " 'comment',\n",
       " 'wakeup',\n",
       " 'besid',\n",
       " 'dirti',\n",
       " 'sex',\n",
       " 'lmaooo',\n",
       " '😤',\n",
       " 'loui',\n",
       " 'throw',\n",
       " 'caus',\n",
       " 'inspir',\n",
       " 'ff',\n",
       " 'twoof',\n",
       " 'gr',\n",
       " 'wkend',\n",
       " 'kind',\n",
       " 'exhaust',\n",
       " 'word',\n",
       " 'cheltenham',\n",
       " 'area',\n",
       " 'kale',\n",
       " 'crisp',\n",
       " 'ruin',\n",
       " 'open',\n",
       " 'worldwid',\n",
       " 'outta',\n",
       " 'sfvbeta',\n",
       " 'vantast',\n",
       " 'xcylin',\n",
       " 'bundl',\n",
       " 'show',\n",
       " 'internet',\n",
       " 'price',\n",
       " 'realisticli',\n",
       " 'pay',\n",
       " 'net',\n",
       " 'educ',\n",
       " 'power',\n",
       " 'weapon',\n",
       " 'nelson',\n",
       " 'mandela',\n",
       " 'recent',\n",
       " 'j',\n",
       " 'chenab',\n",
       " 'flow',\n",
       " 'pakistan',\n",
       " 'incredibleindia',\n",
       " 'teenchoic',\n",
       " 'choiceinternationalartist',\n",
       " 'superjunior',\n",
       " 'caught',\n",
       " 'first',\n",
       " 'salmon',\n",
       " 'super-blend',\n",
       " 'project',\n",
       " 'youth@bipolaruk.org.uk',\n",
       " 'awesom',\n",
       " 'stream',\n",
       " 'alma',\n",
       " 'mater',\n",
       " 'highschoolday',\n",
       " 'clientvisit',\n",
       " 'faith',\n",
       " 'christian',\n",
       " 'school',\n",
       " 'lizaminnelli',\n",
       " 'upcom',\n",
       " 'uk',\n",
       " '😄',\n",
       " 'singl',\n",
       " 'hill',\n",
       " 'everi',\n",
       " 'beat',\n",
       " 'wrong',\n",
       " 'readi',\n",
       " 'natur',\n",
       " 'pefumeri',\n",
       " 'workshop',\n",
       " 'neal',\n",
       " 'yard',\n",
       " 'covent',\n",
       " 'tomorrow',\n",
       " 'fback',\n",
       " 'indo',\n",
       " 'harmo',\n",
       " 'americano',\n",
       " 'rememb',\n",
       " 'aww',\n",
       " 'head',\n",
       " 'saw',\n",
       " 'dark',\n",
       " 'handshom',\n",
       " 'juga',\n",
       " 'hurray',\n",
       " 'hate',\n",
       " 'cant',\n",
       " 'decid',\n",
       " 'save',\n",
       " 'list',\n",
       " 'hiya',\n",
       " 'exec',\n",
       " 'loryn.good@lincs-chamber.co.uk',\n",
       " 'photo',\n",
       " 'thx',\n",
       " 'china',\n",
       " 'homosexu',\n",
       " 'hyungbot',\n",
       " 'give',\n",
       " 'fam',\n",
       " 'mind',\n",
       " 'timetunnel',\n",
       " 'quit',\n",
       " 'radio',\n",
       " 'set',\n",
       " 'heart',\n",
       " 'hiii',\n",
       " 'jack',\n",
       " 'ili',\n",
       " '✨',\n",
       " 'domino',\n",
       " 'pub',\n",
       " 'heat',\n",
       " 'prob',\n",
       " 'sorri',\n",
       " 'hastili',\n",
       " 'type',\n",
       " 'came',\n",
       " 'pakistani',\n",
       " 'dreamteam',\n",
       " 'gooo',\n",
       " 'bailey',\n",
       " 'pbbgold',\n",
       " 'drank',\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad8ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
