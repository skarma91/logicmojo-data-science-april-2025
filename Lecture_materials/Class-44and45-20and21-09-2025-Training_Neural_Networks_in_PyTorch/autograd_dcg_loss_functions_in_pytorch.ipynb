{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122e03de",
   "metadata": {},
   "source": [
    "## Autograd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c731776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad   # importing the auto-grad function\n",
    "import torch.nn.functional as F   # importing functional module, which will help us using activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93801ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True)  # setting requires_grad=True to track computation with it\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "a = F.sigmoid(x*w + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21cdbe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9991], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93d3589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0027])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(a, w, retain_graph=True)[0]   # retain_graph=True to retain the computation graph for further computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c049879",
   "metadata": {},
   "source": [
    "Above, the retain_graph=True means the computation graph will be kept in memory -- this is for example purposes so that we can use the grad function again below. In practice, we usually want to free the computation graph in every round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22140765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0009])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(a, b, retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3dc8b1",
   "metadata": {},
   "source": [
    "### Automatic way of computing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c834c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b514b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0082])\n",
      "tensor([0.0027])\n"
     ]
    }
   ],
   "source": [
    "a.backward(retain_graph=True)   # uses chain rule to compute gradient for all the tensors (with requires_grad=True) involved in the computation of 'a'\n",
    "print(w.grad)  # printing the gradients d(a)/d(w)\n",
    "print(b.grad)  # printing the gradients d(a)/d(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258110e",
   "metadata": {},
   "source": [
    "## What is accumulation of gradient?\n",
    "\n",
    "By default, PyTorch accumulates gradients into `.grad` every time you call `.backward()`.\n",
    "\n",
    "Because sometimes we want to sum gradients across multiple backward passes before updating weights.\n",
    "\n",
    "Think of it as “collecting all contributions to the gradient” before taking a step.\n",
    "\n",
    "For example:\n",
    "\n",
    "**1. Simulating Large Batch Training (Gradient Accumulation)**\n",
    "\n",
    "- Suppose your GPU can only handle a batch size of 32.\n",
    "\n",
    "- But you want to train as if batch size = 128 (for more stable gradients).\n",
    "\n",
    "- You can split the batch into 4 mini-batches, run forward + backward on each, accumulate the gradients, and then do gradient descent once.\n",
    "\n",
    "This way, memory usage is small, but effective batch size is large.\n",
    "\n",
    "**2. Multi-Loss Training (Multiple Objectives)**\n",
    "\n",
    "- Imagine training a model with two loss functions (say, classification + reconstruction).\n",
    "\n",
    "- You compute gradients from the first loss (loss1.backward()), then from the second loss (loss2.backward()), and accumulate both before performing gradient descent.\n",
    "\n",
    "This way, both objectives influence the weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b196380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-1 (z1): tensor([15.], grad_fn=<MulBackward0>), output-2 (z2): tensor([58.], grad_fn=<AddBackward0>)\n",
      "Gradient of x for z1: tensor([3.]), Gradient of y for z1: tensor([5.])\n",
      "Gradient of x: tensor([19.]), Gradient of y: tensor([15.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "z1 = x * y\n",
    "z2 = x**2 + 2* (x * y) + 3\n",
    "\n",
    "print(f\"output-1 (z1): {z1}, output-2 (z2): {z2}\")\n",
    "\n",
    "z1.backward()  # computing gradient for out1\n",
    "print(f\"Gradient of x for z1: {x.grad}, Gradient of y for z1: {y.grad}\")\n",
    "\n",
    "z2.backward()  # computing gradient for out2\n",
    "print(f\"Gradient of x: {x.grad}, Gradient of y: {y.grad}\")  # gradients will be accumulated here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958ad64",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "$$ z_1 = xy \\text{  and  } z_2 = x^2 + 2xy + 3 $$\n",
    "$$ \\frac{\\partial z_1}{\\partial x} = y \\text{  and  } \\frac{\\partial z_2}{\\partial x} = 2x + 2y $$\n",
    "$$ \\frac{\\partial z_1}{\\partial y} = x \\text{  and  } \\frac{\\partial z_2}{\\partial y} = 2x $$\n",
    "\n",
    "For the given values $x = 5$ and $x = 3$\n",
    "\n",
    "$$ \\frac{\\partial z_1}{\\partial x} = 3 \\text{  and  } \\frac{\\partial z_2}{\\partial x} = 16 $$\n",
    "$$ \\frac{\\partial z_1}{\\partial y} = 5 \\text{  and  } \\frac{\\partial z_2}{\\partial y} = 10 $$\n",
    "\n",
    "- When you call `z1.backward()` it computes the gradient of `z1` w.r.t. `x` and `y`\n",
    "- Again when you call  `z2.backward()` it computes the gradient of `z2` w.r.t. `x` and `y` and add these gradients with the previously computed gradients of `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcfc862",
   "metadata": {},
   "source": [
    "## How to stop accumulating gradients?\n",
    "\n",
    "In most training loops, you want to reset (zero-out) the gradients before the next iteration.\n",
    "\n",
    "One of the way to stop accumulating the gradients is to set the gradients of the variables to none or zero manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc3820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x for z1: tensor([3.]), Gradient of y for z1: tensor([5.])\n",
      "Gradient of x for z2: tensor([16.]), Gradient of y for z2: tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "# manually clear the gradients\n",
    "\n",
    "x = torch.tensor([5.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "z1 = x * y\n",
    "z2 = x**2 + 2* (x * y) + 3\n",
    "\n",
    "z1.backward()  # computing gradient for out1\n",
    "print(f\"Gradient of x for z1: {x.grad}, Gradient of y for z1: {y.grad}\")\n",
    "\n",
    "x.grad = None  # x.grad.zero_() can also be used to reset the gradients\n",
    "y.grad = None\n",
    "\n",
    "z2.backward()  # computing gradient for out2\n",
    "print(f\"Gradient of x for z2: {x.grad}, Gradient of y for z2: {y.grad}\")  # gradients will not be accumulated here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295deaf3",
   "metadata": {},
   "source": [
    "## Dynamic Computation Graph (Define-by-Run)\n",
    "\n",
    "In PyTorch, the computation graph is dynamic — meaning it is built on the fly as operations are executed.\n",
    "This contrasts with frameworks like TensorFlow (v1.x) that used a static computation graph, where you first define the whole graph and then execute it.\n",
    "\n",
    "In PyTorch:\n",
    "\n",
    "- Every time you perform an operation (like addition, multiplication, matrix multiplication), a node is added to the computation graph.\n",
    "\n",
    "- The graph is re-built each time you call `forward`.\n",
    "\n",
    "- This is why it’s called “define-by-run”: the graph is defined dynamically while running your code.\n",
    "\n",
    "This gives flexibility, especially useful for:\n",
    "\n",
    "- Variable-length sequences (e.g., time series / sentences with different sequence lengths).\n",
    "\n",
    "- Conditional computations (`if`, `for`, etc.) inside the forward pass.\n",
    "\n",
    "- Debugging (since you can use Python control flow directly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e768a19",
   "metadata": {},
   "source": [
    "### Example 1: Basic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dee69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-0.1577,  0.5581, -2.5103], requires_grad=True)\n",
      "y: tensor([0.3114, 0.6489, 0.7322], requires_grad=True)\n",
      "z: tensor([-0.0491,  0.3621, -1.8381], grad_fn=<MulBackward0>)\n",
      "out: tensor(-1.5251, grad_fn=<SumBackward0>)\n",
      "Gradient of x: tensor([0.3114, 0.6489, 0.7322])\n",
      "Gradient of y: tensor([-0.1577,  0.5581, -2.5103])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with gradient tracking\n",
    "x = torch.randn(3, requires_grad=True)  # random tensor of shape (3,)\n",
    "y = torch.randn(3, requires_grad=True)  # random tensor of shape (3,)\n",
    "\n",
    "# Forward computation\n",
    "z = x * y   # operation builds part of graph\n",
    "out = z.sum()  # final scalar output\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n",
    "print(\"out:\", out)\n",
    "\n",
    "# Backward pass\n",
    "out.backward()\n",
    "\n",
    "print(\"Gradient of x:\", x.grad)\n",
    "print(\"Gradient of y:\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75636012",
   "metadata": {},
   "source": [
    "- When `z = x * y` is executed, PyTorch dynamically creates a node for elementwise multiplication.\n",
    "\n",
    "- When `out = z.sum()` is executed, another node for summation is added.\n",
    "\n",
    "- Finally, `out.backward()` traverses the graph backwards to compute gradients.\n",
    "\n",
    "- If you run the forward pass again with different shapes or conditions, PyTorch will build a new graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e5eea",
   "metadata": {},
   "source": [
    "### Example 2: Dynamic Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0c5374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, requires_grad = True)\n",
    "y = torch.randn(4, requires_grad = True)\n",
    "\n",
    "\n",
    "def f(a, b, flag=True):\n",
    "    if flag:\n",
    "        # one type of computation\n",
    "        c = a * b\n",
    "    else:\n",
    "        # another type of computation\n",
    "        c = a + b\n",
    "\n",
    "    return c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "840bdf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.1173, -1.3290,  1.7495, -1.2362], requires_grad=True)\n",
      "y: tensor([ 0.5248, -0.7247,  0.4923,  0.3318], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"x:\", x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b08f077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output with flag=True: 1.4759141206741333, output with flag=False: -0.07416081428527832\n"
     ]
    }
   ],
   "source": [
    "out1 = f(x, y, True)\n",
    "out2 = f(x, y, False)\n",
    "\n",
    "print(f\"output with flag=True: {out1}, output with flag=False: {out2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461e55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x after out1.backward(): tensor([ 0.5248, -0.7247,  0.4923,  0.3318])\n"
     ]
    }
   ],
   "source": [
    "out1.backward()\n",
    "\n",
    "print(\"Gradient of x after out1.backward():\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d16d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x after out2.backward(): tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad = None  # resetting the gradients of x, otherwise they will accumulate\n",
    "\n",
    "out2.backward()\n",
    "\n",
    "print(\"Gradient of x after out2.backward():\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886cc829",
   "metadata": {},
   "source": [
    "## Loss functions in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f62495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df08b4",
   "metadata": {},
   "source": [
    "### MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28437d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.2300\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0])\n",
    "targets = torch.tensor([0.6, 1.8, 2.3])\n",
    "\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "loss_mse = mse_loss_fn(inputs, targets)\n",
    "\n",
    "print(f\"MSE Loss: {loss_mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78593daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss in functional representation: 0.2300\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE Loss in functional representation: {F.mse_loss(inputs, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df723821",
   "metadata": {},
   "source": [
    "### BCE and BCE with logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6419c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8808, 0.1824, 0.7685])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([2.0, -1.5, 1.2])\n",
    "\n",
    "targets = torch.tensor([1, 1, 0], dtype=torch.float32)\n",
    "\n",
    "probas = F.sigmoid(logits)\n",
    "\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d92724e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss: 1.0972\n",
      "BCE loss in functional representation: 1.0972\n"
     ]
    }
   ],
   "source": [
    "bce_loss_fn = nn.BCELoss()\n",
    "\n",
    "loss_bce = bce_loss_fn(probas, targets) # passing probas (post sigmoid) as inputs\n",
    "\n",
    "print(f\"BCE loss: {loss_bce.item():.4f}\")\n",
    "\n",
    "print(f\"BCE loss in functional representation: {F.binary_cross_entropy(probas, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbc551ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE with logit loss: 1.0972\n",
      "BCE with logit loss in functional representation: 1.0972\n"
     ]
    }
   ],
   "source": [
    "bce_with_logits_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "loss_bce_with_logits = bce_with_logits_loss_fn(logits, targets) # passing logits (pre-sigmoid) as inputs (not the probas)\n",
    "\n",
    "print(f\"BCE with logit loss: {loss_bce_with_logits.item():.4f}\")\n",
    "\n",
    "print(f\"BCE with logit loss in functional representation: {F.binary_cross_entropy_with_logits(logits, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fdf38",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab8c03c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]), torch.Size([4]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([[1.2, 2.3, -0.5],\n",
    "                       [3.1, 1.8, 0.9],\n",
    "                       [2.8, -1.6, 4.9],\n",
    "                       [0.5, 2.2, 1.9]])\n",
    "\n",
    "targets = torch.tensor([1, 0, 2, 1])\n",
    "\n",
    "logits.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68f9807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2388, 0.7175, 0.0436],\n",
      "        [0.7229, 0.1970, 0.0801],\n",
      "        [0.1090, 0.0013, 0.8897],\n",
      "        [0.0950, 0.5199, 0.3851]])\n",
      "Sum of probabilities across classes (should be 1.0 for each sample): tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "probas = F.softmax(logits, dim=1) \n",
    "\n",
    "print(probas)\n",
    "\n",
    "print(f\"Sum of probabilities across classes (should be 1.0 for each sample): {probas.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb56ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss: 0.3569\n",
      "CE loss in functional representation: 0.3569\n"
     ]
    }
   ],
   "source": [
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_ce = ce_loss_fn(logits, targets) # passing logits (pre softmax) as inputs\n",
    "\n",
    "print(f\"CE loss: {loss_ce.item():.4f}\")\n",
    "\n",
    "print(f\"CE loss in functional representation: {F.cross_entropy(logits, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067e517",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34fdec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4319, -0.3319, -3.1319],\n",
      "        [-0.3245, -1.6245, -2.5245],\n",
      "        [-2.2169, -6.6169, -0.1169],\n",
      "        [-2.3541, -0.6541, -0.9541]])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log_softmax(logits, dim=1)\n",
    "\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "974cbae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL loss: 0.3569\n",
      "NLL loss in functional representation: 0.3569\n"
     ]
    }
   ],
   "source": [
    "nll_loss_fn = nn.NLLLoss()\n",
    "\n",
    "loss_nll = nll_loss_fn(log_probas, targets) # passing log-probas (log softmax) as inputs\n",
    "\n",
    "print(f\"NLL loss: {loss_nll.item():.4f}\")\n",
    "\n",
    "print(f\"NLL loss in functional representation: {F.nll_loss(log_probas, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7075eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
