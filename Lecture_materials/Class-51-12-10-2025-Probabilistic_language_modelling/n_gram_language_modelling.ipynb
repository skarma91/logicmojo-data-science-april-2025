{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f78ec31",
   "metadata": {},
   "source": [
    "# Language Modeling on Shakespeare’s Works Using N-Grams\n",
    "\n",
    "## Objective:\n",
    "The goal of this project is to build and evaluate unigram, bigram, and trigram language models using the Complete Works of William Shakespeare. We aim to:\n",
    "\n",
    "- Train statistical n-gram models on the text corpus.\n",
    "\n",
    "- Compute perplexity to evaluate model performance.\n",
    "\n",
    "- Generate sentences by sampling from these models.\n",
    "\n",
    "## Dataset:\n",
    "We use the publicly available dataset from Kaggle: [Shakespeare Online - Complete Works](https://www.kaggle.com/datasets/kewagbln/shakespeareonline)\n",
    "\n",
    "The dataset contains a single .txt file with all of Shakespeare's plays, poems, and sonnets.\n",
    "\n",
    "The raw corpus is tokenized and split into train and test sets for evaluation.\n",
    "\n",
    "## Tasks Overview:\n",
    "- Data Preprocessing – Load and tokenize the Shakespeare corpus.\n",
    "\n",
    "- Train N-Gram Models – Unigram, Bigram, and Trigram with Add-1 smoothing.\n",
    "\n",
    "- Evaluate – Compute perplexity scores on the test set.\n",
    "\n",
    "- Generate Sentences – Use probabilistic sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db792f",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ba25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc595101",
   "metadata": {},
   "source": [
    "## Tokenization¶\n",
    "\n",
    "We define a custom tokenizer simple_tokenize to convert the raw Shakespeare text into a list of tokens (words). This function:\n",
    "\n",
    "- Converts all text to lowercase to ensure uniformity.\n",
    "\n",
    "- Uses regular expressions `(r\"\\b\\w+\\b\")` to extract word tokens while ignoring punctuation.\n",
    "\n",
    "This basic tokenization will help us break down the dataset into individual words for training the n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4053ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fad631",
   "metadata": {},
   "source": [
    "## Print first few lines of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c44548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 5458199 characters\n",
      "this is the 100th etext file presented by project gutenberg, and\n",
      "is presented in cooperation with world library, inc., from their\n",
      "library of the future and shakespeare cdroms.  project gutenberg\n",
      "often releases etexts that are not placed in the public domain!!\n",
      "\n",
      "shakespeare\n",
      "\n",
      "*this etext has certain copyright implications you should read!*\n",
      "\n",
      "<<this electronic version of the complete works of william\n",
      "shakespeare is copyright 1990-1993 by world library, inc., and is\n",
      "provided by project gutenberg etext of illinois benedictine college\n",
      "with permission.  electronic and machine readable copies may be\n",
      "distributed so long as such copies (1) are for your or others\n",
      "personal use only, and (2) are not distributed or used\n",
      "commercially.  prohibited commercial distribution includes by any\n",
      "service that charges for download time or for membership.>>\n",
      "\n",
      "*project gutenberg is proud to cooperate with the world library*\n",
      "in the presentation of the complete works of william shakespeare\n",
      "for your reading for education and entertainment.  however, this\n",
      "is neither shareware nor public domain. . .and under the library\n",
      "of the future conditions of this presentation. . .no charges may\n",
      "be made for *any* access to this material.  you are encouraged!!\n",
      "to give it away to anyone you like, but no charges are allowed!!\n",
      "\n",
      "\n",
      "**welcome to the world of free plain vanilla electronic texts**\n",
      "\n",
      "**etexts readable by both humans and by computers, since 1971**\n",
      "\n",
      "*these etexts prepared by hundreds of volunteers and donations*\n",
      "\n",
      "information on contacting project gutenberg to get etexts, and\n",
      "further information is included below.  we need your donations.\n",
      "\n",
      "\n",
      "the complete works of william shakespeare \n",
      "\n",
      "january, 1994  [etext #100]\n",
      "\n",
      "\n",
      "the library of the future complete works of william shakespeare \n",
      "library of the future is a trademark (tm) of world library inc.\n",
      "******this file should be named shaks12.txt or shaks12.zip*****\n",
      "\n",
      "corrected editions of our etexts get a new number, shaks13.txt\n",
      "versions based on separate sources get new letter, shaks10a.txt\n",
      "\n",
      "if you would like further information about world library, inc.\n",
      "please call them at 1-800-443-0238 or email julianc@netcom.com\n",
      "please give them our thanks for their shakespeare cooperation!\n",
      "\n",
      "\n",
      "the official release date of all project gutenberg etexts is at\n",
      "midnight, central time, of the last day of the stated month.  a\n",
      "preliminary version may often be posted for suggestion, comment\n",
      "and editing by those who wish to do so.  to be sure you have an\n",
      "up to date first edition [xxxxx10x.xxx] please check file sizes\n",
      "in the first week of the next month.  since our ftp program has\n",
      "a bug in it that scrambles the date [tried to fix and failed] a\n",
      "look at the file size will have to do, but we will try to see a\n",
      "new copy has at least one byte more or less.\n",
      "\n",
      "\n",
      "information about project gutenberg (one page)\n",
      "\n",
      "we produce about two million dollars for each hour we work.  the\n",
      "fifty hours is one conservative estimate for how long it we take\n",
      "to get any etext selected, entered, proofread, edited, copyright\n",
      "searched and analyzed, the copyright letters written, etc.  this\n",
      "projected audience is one hundred million readers.  if our value\n",
      "per text is nominally estimated at one dollar, then we produce 2\n",
      "million dollars per hour this year we, will have to do four text\n",
      "files per month:  thus upping our productivity from one million.\n",
      "the goal of project gutenberg is to give away one trillion etext\n",
      "files by the december 31, 2001.  [10,000 x 100,000,000=trillion]\n",
      "this is ten thousand titles each to one hundred million readers,\n",
      "which is 10% of the expected number of computer users by the end\n",
      "of the year 2001.\n",
      "\n",
      "we need your donations more than ever!\n",
      "\n",
      "all donations should be made to \"project gutenberg/ibc\", and are\n",
      "tax deductible to the extent allowable by law (\"ibc\" is illinois\n",
      "benedictine college).  (subscriptions to our paper newsletter go\n",
      "to ibc, too)\n",
      "\n",
      "for these and other matters, please mail to:\n",
      "\n",
      "project gutenberg\n",
      "p. o. box  2782\n",
      "champaign, il 61825\n",
      "\n",
      "when all other email fails try our michael s. hart, executive director:\n",
      "hart@vmd.cso.uiuc.edu (internet)   hart@uiucvmd   (bitnet)\n",
      "\n",
      "we would prefer to send you this information by email\n",
      "(internet, bitnet, compuserve, attmail or mcimail).\n",
      "\n",
      "******\n",
      "if you have an ftp program (or emulator), please\n",
      "ftp directly to the project gutenberg archives:\n",
      "[mac users, do not point and click. . .type]\n",
      "\n",
      "ftp mrcnext.cso.uiuc.edu\n",
      "login:  anonymous\n",
      "password:  your@login\n",
      "cd etext/etext91\n",
      "or cd etext92\n",
      "or cd etext93 [for new books]  [now also in cd etext/etext93]\n",
      "or cd etext/articles [get suggest gut for more information]\n",
      "dir [to see files]\n",
      "get or mget [to get files. . .set bin for zip files]\n",
      "get 0index.gut\n",
      "for a list of books\n",
      "and\n",
      "get new gut for general information\n",
      "and\n",
      "mget gut* for newsletters.\n",
      "\n",
      "**information prepared by the project gutenberg legal advisor**\n",
      "\n",
      "\n",
      "***** small print! for complete shakespeare *****\n",
      "\n",
      "this electronic version of the complete works of william\n",
      "shakespeare is copyright 1990-1993 by world library, inc.,\n",
      "and is provided\n"
     ]
    }
   ],
   "source": [
    "with open(\"./t8.shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "print(f\"Corpus length: {len(text)} characters\")\n",
    "print(text[:5000]) # Print first 5000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b37ea",
   "metadata": {},
   "source": [
    "## Load and Split the Dataset\n",
    "\n",
    "We define the load_and_split function to:\n",
    "\n",
    "- Open and read the raw Shakespeare text file.\n",
    "- Convert the entire content to lowercase.\n",
    "- Use the simple_tokenize() function to break the text into tokens.\n",
    "- Split the tokens into two parts:\n",
    "\n",
    "    - Training set (default 80%)\n",
    "    - Testing set (remaining 20%)\n",
    "\n",
    "This prepares the data for building and evaluating the language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1109cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(path, split_ratio=0.8):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    # Limit size for memory safety (optional)\n",
    "    # text = text[:500000]  # First 500K characters\n",
    "\n",
    "    tokens = simple_tokenize(text)\n",
    "    split_point = int(len(tokens) * split_ratio)\n",
    "    return tokens[:split_point], tokens[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44069b",
   "metadata": {},
   "source": [
    "## Define the N-Gram Language Model¶\n",
    "\n",
    "The `NGramModel` class implements a basic statistical language model for any value of n (unigram, bigram, trigram, etc.).\n",
    "\n",
    "Methods:\n",
    "\n",
    "**\\_\\_init\\_\\_()**: Initializes the model with:\n",
    "\n",
    "- `n`: the size of the n-gram (e.g., 1, 2, or 3).\n",
    "\n",
    "- `ngram_counts`: a nested dictionary to count n-gram occurrences.\n",
    "\n",
    "- `context_counts`: tracks total counts of each context (prefix of length n-1).\n",
    "\n",
    "- `vocab`: a set of all unique tokens.\n",
    "\n",
    "**train(tokens)**\n",
    "\n",
    "- Adds `<s>` (start) and `</s>` (end) tokens to each sentence.\n",
    "- Builds the n-gram and context frequency tables from the training data.\n",
    "\n",
    "**get_prob(context, word, alpha=1.0)**\n",
    "\n",
    "- Computes the probability of a word given its context using **Laplace (Add-1) smoothing**:\n",
    "\n",
    "$$ P(w∣context) = \\frac{count(context, w)+ \\alpha}{count(context)+ \\alpha \\cdot V} $$\n",
    "\n",
    "where $V$ is the vocabulary size.\n",
    "\n",
    "**perplexity(tokens)**\n",
    "\n",
    "- Computes the perplexity score for a given sequence of tokens using:\n",
    "\n",
    "$$ Perplexity = e^{-\\frac{1}{N}\\sum_{i=1}^{N}log P(w_i | context)} $$\n",
    "\n",
    "- Perplexity indicates how well the model predicts the test data: lower is better.\n",
    "\n",
    "> We handle prob = 0 cases by applying a penalty (float('inf')) to avoid undefined logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801757ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, tokens):\n",
    "        tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "        self.vocab.update(tokens)\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            context = tuple(tokens[i:i + self.n - 1])\n",
    "            word = tokens[i + self.n - 1]\n",
    "            self.ngram_counts[context][word] += 1\n",
    "            self.context_counts[context] += 1\n",
    "\n",
    "    def get_prob(self, context, word, alpha=1.0):\n",
    "        context = tuple(context)\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        # Laplace smoothing\n",
    "        count = self.ngram_counts[context][word]\n",
    "        total = self.context_counts[context]\n",
    "        return (count + alpha) / (total + alpha * vocab_size)\n",
    "\n",
    "\n",
    "    def perplexity(self, tokens):\n",
    "        tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            context = tokens[i:i + self.n - 1]\n",
    "            word = tokens[i + self.n - 1]\n",
    "            prob = self.get_prob(context, word)\n",
    "\n",
    "            # Prevent log(0)\n",
    "            if prob > 0:\n",
    "                log_prob_sum += -math.log(prob)\n",
    "            else:\n",
    "                log_prob_sum += float('inf')  # Worst case penalty\n",
    "\n",
    "            N += 1\n",
    "        return math.exp(log_prob_sum / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10aedf2",
   "metadata": {},
   "source": [
    "## Load Dataset and Train N-Gram Models\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- Load and tokenize the Shakespeare corpus using the load_and_split() function.\n",
    "- Split the data into:\n",
    "\n",
    "    - Training tokens (train_tokens)\n",
    "    - Testing tokens (test_tokens)\n",
    "\n",
    "- Initialize and train three language models:\n",
    "\n",
    "    - Unigram (n = 1)\n",
    "    - Bigram (n = 2)\n",
    "    - Trigram (n = 3)\n",
    "\n",
    "Each model learns frequency-based word patterns from the training data using the logic defined in the `NGramModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40159deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split\n",
    "train_tokens, test_tokens = load_and_split(\"./t8.shakespeare.txt\")\n",
    "\n",
    "# Train models\n",
    "unigram = NGramModel(1)\n",
    "bigram = NGramModel(2)\n",
    "trigram = NGramModel(3)\n",
    "\n",
    "unigram.train(train_tokens)\n",
    "bigram.train(train_tokens)\n",
    "trigram.train(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa5a09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " '100th',\n",
       " 'etext',\n",
       " 'file',\n",
       " 'presented',\n",
       " 'by',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'and',\n",
       " 'is',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'cooperation',\n",
       " 'with',\n",
       " 'world',\n",
       " 'library',\n",
       " 'inc',\n",
       " 'from',\n",
       " 'their',\n",
       " 'library',\n",
       " 'of',\n",
       " 'the',\n",
       " 'future',\n",
       " 'and',\n",
       " 'shakespeare',\n",
       " 'cdroms',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'often',\n",
       " 'releases',\n",
       " 'etexts',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'placed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'public',\n",
       " 'domain',\n",
       " 'shakespeare',\n",
       " 'this',\n",
       " 'etext',\n",
       " 'has',\n",
       " 'certain',\n",
       " 'copyright',\n",
       " 'implications',\n",
       " 'you',\n",
       " 'should',\n",
       " 'read',\n",
       " 'this',\n",
       " 'electronic',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complete',\n",
       " 'works',\n",
       " 'of',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'is',\n",
       " 'copyright',\n",
       " '1990',\n",
       " '1993',\n",
       " 'by',\n",
       " 'world',\n",
       " 'library',\n",
       " 'inc',\n",
       " 'and',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'by',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'etext',\n",
       " 'of',\n",
       " 'illinois',\n",
       " 'benedictine',\n",
       " 'college',\n",
       " 'with',\n",
       " 'permission',\n",
       " 'electronic',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'readable',\n",
       " 'copies',\n",
       " 'may',\n",
       " 'be',\n",
       " 'distributed',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'such',\n",
       " 'copies',\n",
       " '1',\n",
       " 'are',\n",
       " 'for',\n",
       " 'your',\n",
       " 'or',\n",
       " 'others',\n",
       " 'personal',\n",
       " 'use',\n",
       " 'only',\n",
       " 'and',\n",
       " '2',\n",
       " 'are',\n",
       " 'not',\n",
       " 'distributed',\n",
       " 'or',\n",
       " 'used',\n",
       " 'commercially',\n",
       " 'prohibited',\n",
       " 'commercial',\n",
       " 'distribution',\n",
       " 'includes',\n",
       " 'by',\n",
       " 'any',\n",
       " 'service',\n",
       " 'that',\n",
       " 'charges',\n",
       " 'for',\n",
       " 'download',\n",
       " 'time',\n",
       " 'or',\n",
       " 'for',\n",
       " 'membership',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'is',\n",
       " 'proud',\n",
       " 'to',\n",
       " 'cooperate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'world',\n",
       " 'library',\n",
       " 'in',\n",
       " 'the',\n",
       " 'presentation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complete',\n",
       " 'works',\n",
       " 'of',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'for',\n",
       " 'your',\n",
       " 'reading',\n",
       " 'for',\n",
       " 'education',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " 'however',\n",
       " 'this',\n",
       " 'is',\n",
       " 'neither',\n",
       " 'shareware',\n",
       " 'nor',\n",
       " 'public',\n",
       " 'domain',\n",
       " 'and',\n",
       " 'under',\n",
       " 'the',\n",
       " 'library',\n",
       " 'of',\n",
       " 'the',\n",
       " 'future',\n",
       " 'conditions',\n",
       " 'of',\n",
       " 'this',\n",
       " 'presentation',\n",
       " 'no',\n",
       " 'charges',\n",
       " 'may',\n",
       " 'be',\n",
       " 'made',\n",
       " 'for',\n",
       " 'any',\n",
       " 'access',\n",
       " 'to',\n",
       " 'this',\n",
       " 'material',\n",
       " 'you',\n",
       " 'are',\n",
       " 'encouraged',\n",
       " 'to',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'to',\n",
       " 'anyone',\n",
       " 'you',\n",
       " 'like',\n",
       " 'but',\n",
       " 'no',\n",
       " 'charges',\n",
       " 'are',\n",
       " 'allowed',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'free',\n",
       " 'plain',\n",
       " 'vanilla',\n",
       " 'electronic',\n",
       " 'texts',\n",
       " 'etexts',\n",
       " 'readable',\n",
       " 'by',\n",
       " 'both',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'by',\n",
       " 'computers',\n",
       " 'since',\n",
       " '1971',\n",
       " 'these',\n",
       " 'etexts',\n",
       " 'prepared',\n",
       " 'by',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'volunteers',\n",
       " 'and',\n",
       " 'donations',\n",
       " 'information',\n",
       " 'on',\n",
       " 'contacting',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'to',\n",
       " 'get',\n",
       " 'etexts',\n",
       " 'and',\n",
       " 'further',\n",
       " 'information',\n",
       " 'is',\n",
       " 'included',\n",
       " 'below',\n",
       " 'we',\n",
       " 'need',\n",
       " 'your',\n",
       " 'donations',\n",
       " 'the',\n",
       " 'complete',\n",
       " 'works',\n",
       " 'of',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'january',\n",
       " '1994',\n",
       " 'etext',\n",
       " '100',\n",
       " 'the',\n",
       " 'library',\n",
       " 'of',\n",
       " 'the',\n",
       " 'future',\n",
       " 'complete',\n",
       " 'works',\n",
       " 'of',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'library',\n",
       " 'of',\n",
       " 'the',\n",
       " 'future',\n",
       " 'is',\n",
       " 'a',\n",
       " 'trademark',\n",
       " 'tm',\n",
       " 'of',\n",
       " 'world',\n",
       " 'library',\n",
       " 'inc',\n",
       " 'this',\n",
       " 'file',\n",
       " 'should',\n",
       " 'be',\n",
       " 'named',\n",
       " 'shaks12',\n",
       " 'txt',\n",
       " 'or',\n",
       " 'shaks12',\n",
       " 'zip',\n",
       " 'corrected',\n",
       " 'editions',\n",
       " 'of',\n",
       " 'our',\n",
       " 'etexts',\n",
       " 'get',\n",
       " 'a',\n",
       " 'new',\n",
       " 'number',\n",
       " 'shaks13',\n",
       " 'txt',\n",
       " 'versions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'separate',\n",
       " 'sources',\n",
       " 'get',\n",
       " 'new',\n",
       " 'letter',\n",
       " 'shaks10a',\n",
       " 'txt',\n",
       " 'if',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'further',\n",
       " 'information',\n",
       " 'about',\n",
       " 'world',\n",
       " 'library',\n",
       " 'inc',\n",
       " 'please',\n",
       " 'call',\n",
       " 'them',\n",
       " 'at',\n",
       " '1',\n",
       " '800',\n",
       " '443',\n",
       " '0238',\n",
       " 'or',\n",
       " 'email',\n",
       " 'julianc',\n",
       " 'netcom',\n",
       " 'com',\n",
       " 'please',\n",
       " 'give',\n",
       " 'them',\n",
       " 'our',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'their',\n",
       " 'shakespeare',\n",
       " 'cooperation',\n",
       " 'the',\n",
       " 'official',\n",
       " 'release',\n",
       " 'date',\n",
       " 'of',\n",
       " 'all',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'etexts',\n",
       " 'is',\n",
       " 'at',\n",
       " 'midnight',\n",
       " 'central',\n",
       " 'time',\n",
       " 'of',\n",
       " 'the',\n",
       " 'last',\n",
       " 'day',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stated',\n",
       " 'month',\n",
       " 'a',\n",
       " 'preliminary',\n",
       " 'version',\n",
       " 'may',\n",
       " 'often',\n",
       " 'be',\n",
       " 'posted',\n",
       " 'for',\n",
       " 'suggestion',\n",
       " 'comment',\n",
       " 'and',\n",
       " 'editing',\n",
       " 'by',\n",
       " 'those',\n",
       " 'who',\n",
       " 'wish',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sure',\n",
       " 'you',\n",
       " 'have',\n",
       " 'an',\n",
       " 'up',\n",
       " 'to',\n",
       " 'date',\n",
       " 'first',\n",
       " 'edition',\n",
       " 'xxxxx10x',\n",
       " 'xxx',\n",
       " 'please',\n",
       " 'check',\n",
       " 'file',\n",
       " 'sizes',\n",
       " 'in',\n",
       " 'the',\n",
       " 'first',\n",
       " 'week',\n",
       " 'of',\n",
       " 'the',\n",
       " 'next',\n",
       " 'month',\n",
       " 'since',\n",
       " 'our',\n",
       " 'ftp',\n",
       " 'program',\n",
       " 'has',\n",
       " 'a',\n",
       " 'bug',\n",
       " 'in',\n",
       " 'it',\n",
       " 'that',\n",
       " 'scrambles',\n",
       " 'the',\n",
       " 'date',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'fix',\n",
       " 'and',\n",
       " 'failed',\n",
       " 'a',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'file',\n",
       " 'size',\n",
       " 'will',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'but',\n",
       " 'we',\n",
       " 'will',\n",
       " 'try',\n",
       " 'to',\n",
       " 'see',\n",
       " 'a',\n",
       " 'new',\n",
       " 'copy',\n",
       " 'has',\n",
       " 'at',\n",
       " 'least',\n",
       " 'one',\n",
       " 'byte',\n",
       " 'more',\n",
       " 'or',\n",
       " 'less',\n",
       " 'information',\n",
       " 'about',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'one',\n",
       " 'page',\n",
       " 'we',\n",
       " 'produce',\n",
       " 'about',\n",
       " 'two',\n",
       " 'million',\n",
       " 'dollars',\n",
       " 'for',\n",
       " 'each',\n",
       " 'hour',\n",
       " 'we',\n",
       " 'work',\n",
       " 'the',\n",
       " 'fifty',\n",
       " 'hours',\n",
       " 'is',\n",
       " 'one',\n",
       " 'conservative',\n",
       " 'estimate',\n",
       " 'for',\n",
       " 'how',\n",
       " 'long',\n",
       " 'it',\n",
       " 'we',\n",
       " 'take',\n",
       " 'to',\n",
       " 'get',\n",
       " 'any',\n",
       " 'etext',\n",
       " 'selected',\n",
       " 'entered',\n",
       " 'proofread',\n",
       " 'edited',\n",
       " 'copyright',\n",
       " 'searched',\n",
       " 'and',\n",
       " 'analyzed',\n",
       " 'the',\n",
       " 'copyright',\n",
       " 'letters',\n",
       " 'written',\n",
       " 'etc',\n",
       " 'this',\n",
       " 'projected',\n",
       " 'audience',\n",
       " 'is',\n",
       " 'one',\n",
       " 'hundred',\n",
       " 'million',\n",
       " 'readers',\n",
       " 'if',\n",
       " 'our',\n",
       " 'value',\n",
       " 'per',\n",
       " 'text',\n",
       " 'is',\n",
       " 'nominally',\n",
       " 'estimated',\n",
       " 'at',\n",
       " 'one',\n",
       " 'dollar',\n",
       " 'then',\n",
       " 'we',\n",
       " 'produce',\n",
       " '2',\n",
       " 'million',\n",
       " 'dollars',\n",
       " 'per',\n",
       " 'hour',\n",
       " 'this',\n",
       " 'year',\n",
       " 'we',\n",
       " 'will',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'four',\n",
       " 'text',\n",
       " 'files',\n",
       " 'per',\n",
       " 'month',\n",
       " 'thus',\n",
       " 'upping',\n",
       " 'our',\n",
       " 'productivity',\n",
       " 'from',\n",
       " 'one',\n",
       " 'million',\n",
       " 'the',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'is',\n",
       " 'to',\n",
       " 'give',\n",
       " 'away',\n",
       " 'one',\n",
       " 'trillion',\n",
       " 'etext',\n",
       " 'files',\n",
       " 'by',\n",
       " 'the',\n",
       " 'december',\n",
       " '31',\n",
       " '2001',\n",
       " '10',\n",
       " '000',\n",
       " 'x',\n",
       " '100',\n",
       " '000',\n",
       " '000',\n",
       " 'trillion',\n",
       " 'this',\n",
       " 'is',\n",
       " 'ten',\n",
       " 'thousand',\n",
       " 'titles',\n",
       " 'each',\n",
       " 'to',\n",
       " 'one',\n",
       " 'hundred',\n",
       " 'million',\n",
       " 'readers',\n",
       " 'which',\n",
       " 'is',\n",
       " '10',\n",
       " 'of',\n",
       " 'the',\n",
       " 'expected',\n",
       " 'number',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'users',\n",
       " 'by',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " '2001',\n",
       " 'we',\n",
       " 'need',\n",
       " 'your',\n",
       " 'donations',\n",
       " 'more',\n",
       " 'than',\n",
       " 'ever',\n",
       " 'all',\n",
       " 'donations',\n",
       " 'should',\n",
       " 'be',\n",
       " 'made',\n",
       " 'to',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ibc',\n",
       " 'and',\n",
       " 'are',\n",
       " 'tax',\n",
       " 'deductible',\n",
       " 'to',\n",
       " 'the',\n",
       " 'extent',\n",
       " 'allowable',\n",
       " 'by',\n",
       " 'law',\n",
       " 'ibc',\n",
       " 'is',\n",
       " 'illinois',\n",
       " 'benedictine',\n",
       " 'college',\n",
       " 'subscriptions',\n",
       " 'to',\n",
       " 'our',\n",
       " 'paper',\n",
       " 'newsletter',\n",
       " 'go',\n",
       " 'to',\n",
       " 'ibc',\n",
       " 'too',\n",
       " 'for',\n",
       " 'these',\n",
       " 'and',\n",
       " 'other',\n",
       " 'matters',\n",
       " 'please',\n",
       " 'mail',\n",
       " 'to',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'p',\n",
       " 'o',\n",
       " 'box',\n",
       " '2782',\n",
       " 'champaign',\n",
       " 'il',\n",
       " '61825',\n",
       " 'when',\n",
       " 'all',\n",
       " 'other',\n",
       " 'email',\n",
       " 'fails',\n",
       " 'try',\n",
       " 'our',\n",
       " 'michael',\n",
       " 's',\n",
       " 'hart',\n",
       " 'executive',\n",
       " 'director',\n",
       " 'hart',\n",
       " 'vmd',\n",
       " 'cso',\n",
       " 'uiuc',\n",
       " 'edu',\n",
       " 'internet',\n",
       " 'hart',\n",
       " 'uiucvmd',\n",
       " 'bitnet',\n",
       " 'we',\n",
       " 'would',\n",
       " 'prefer',\n",
       " 'to',\n",
       " 'send',\n",
       " 'you',\n",
       " 'this',\n",
       " 'information',\n",
       " 'by',\n",
       " 'email',\n",
       " 'internet',\n",
       " 'bitnet',\n",
       " 'compuserve',\n",
       " 'attmail',\n",
       " 'or',\n",
       " 'mcimail',\n",
       " 'if',\n",
       " 'you',\n",
       " 'have',\n",
       " 'an',\n",
       " 'ftp',\n",
       " 'program',\n",
       " 'or',\n",
       " 'emulator',\n",
       " 'please',\n",
       " 'ftp',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'archives',\n",
       " 'mac',\n",
       " 'users',\n",
       " 'do',\n",
       " 'not',\n",
       " 'point',\n",
       " 'and',\n",
       " 'click',\n",
       " 'type',\n",
       " 'ftp',\n",
       " 'mrcnext',\n",
       " 'cso',\n",
       " 'uiuc',\n",
       " 'edu',\n",
       " 'login',\n",
       " 'anonymous',\n",
       " 'password',\n",
       " 'your',\n",
       " 'login',\n",
       " 'cd',\n",
       " 'etext',\n",
       " 'etext91',\n",
       " 'or',\n",
       " 'cd',\n",
       " 'etext92',\n",
       " 'or',\n",
       " 'cd',\n",
       " 'etext93',\n",
       " 'for',\n",
       " 'new',\n",
       " 'books',\n",
       " 'now',\n",
       " 'also',\n",
       " 'in',\n",
       " 'cd',\n",
       " 'etext',\n",
       " 'etext93',\n",
       " 'or',\n",
       " 'cd',\n",
       " 'etext',\n",
       " 'articles',\n",
       " 'get',\n",
       " 'suggest',\n",
       " 'gut',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information',\n",
       " 'dir',\n",
       " 'to',\n",
       " 'see',\n",
       " 'files',\n",
       " 'get',\n",
       " 'or',\n",
       " 'mget',\n",
       " 'to',\n",
       " 'get',\n",
       " 'files',\n",
       " 'set',\n",
       " 'bin',\n",
       " 'for',\n",
       " 'zip',\n",
       " 'files',\n",
       " 'get',\n",
       " '0index',\n",
       " 'gut',\n",
       " 'for',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'books',\n",
       " 'and',\n",
       " 'get',\n",
       " 'new',\n",
       " 'gut',\n",
       " 'for',\n",
       " 'general',\n",
       " 'information',\n",
       " 'and',\n",
       " 'mget',\n",
       " 'gut',\n",
       " 'for',\n",
       " 'newsletters',\n",
       " 'information',\n",
       " 'prepared',\n",
       " 'by',\n",
       " 'the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'legal',\n",
       " 'advisor',\n",
       " 'small',\n",
       " 'print',\n",
       " 'for',\n",
       " 'complete',\n",
       " 'shakespeare',\n",
       " 'this',\n",
       " 'electronic',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complete',\n",
       " 'works',\n",
       " 'of',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'is',\n",
       " 'copyright',\n",
       " '1990',\n",
       " '1993',\n",
       " 'by',\n",
       " 'world',\n",
       " 'library',\n",
       " 'inc',\n",
       " 'and',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'by',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'etext',\n",
       " 'of',\n",
       " 'illinois',\n",
       " 'benedictine',\n",
       " 'college',\n",
       " 'with',\n",
       " 'permission',\n",
       " 'since',\n",
       " 'unlike',\n",
       " 'many',\n",
       " 'other',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'tm',\n",
       " 'etexts',\n",
       " 'this',\n",
       " 'etext',\n",
       " 'is',\n",
       " 'copyright',\n",
       " 'protected',\n",
       " 'and',\n",
       " 'since',\n",
       " 'the',\n",
       " 'materials',\n",
       " 'and',\n",
       " 'methods',\n",
       " 'you',\n",
       " 'use',\n",
       " 'will',\n",
       " 'effect',\n",
       " 'the',\n",
       " 'project',\n",
       " 's',\n",
       " 'reputation',\n",
       " 'your',\n",
       " 'right',\n",
       " 'to',\n",
       " 'copy',\n",
       " 'and',\n",
       " 'distribute',\n",
       " 'it',\n",
       " 'is',\n",
       " 'limited',\n",
       " 'by',\n",
       " 'the',\n",
       " 'copyright',\n",
       " 'and',\n",
       " 'other',\n",
       " 'laws',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'conditions',\n",
       " 'of',\n",
       " 'this',\n",
       " 'small',\n",
       " 'print',\n",
       " 'statement',\n",
       " '1',\n",
       " 'license',\n",
       " 'a',\n",
       " 'you',\n",
       " 'may',\n",
       " 'and',\n",
       " 'are',\n",
       " 'encouraged',\n",
       " 'to',\n",
       " 'distribute',\n",
       " 'electronic',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'readable',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'this',\n",
       " 'etext',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'such',\n",
       " 'copies',\n",
       " '1',\n",
       " 'are',\n",
       " 'for',\n",
       " 'your',\n",
       " 'or',\n",
       " 'others',\n",
       " 'personal',\n",
       " 'use',\n",
       " 'only',\n",
       " 'and',\n",
       " '2',\n",
       " 'are',\n",
       " 'not',\n",
       " 'distributed',\n",
       " 'or',\n",
       " 'used',\n",
       " 'commercially',\n",
       " 'prohibited',\n",
       " 'commercial',\n",
       " 'distribution',\n",
       " 'includes',\n",
       " 'by',\n",
       " 'any',\n",
       " 'service',\n",
       " 'that',\n",
       " 'charges',\n",
       " 'for',\n",
       " 'download',\n",
       " 'time',\n",
       " 'or',\n",
       " 'for',\n",
       " 'membership',\n",
       " 'b',\n",
       " 'this',\n",
       " 'license',\n",
       " 'is',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'the',\n",
       " 'conditions',\n",
       " 'that',\n",
       " 'you',\n",
       " 'honor',\n",
       " 'the',\n",
       " 'refund',\n",
       " 'and',\n",
       " 'replacement',\n",
       " 'provisions',\n",
       " 'of',\n",
       " 'this',\n",
       " 'small',\n",
       " 'print',\n",
       " 'statement',\n",
       " 'and',\n",
       " 'that',\n",
       " 'you',\n",
       " 'distribute',\n",
       " 'exact',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'this',\n",
       " 'etext',\n",
       " 'including',\n",
       " 'this',\n",
       " 'small',\n",
       " 'print',\n",
       " 'statement',\n",
       " 'such',\n",
       " 'copies',\n",
       " 'can',\n",
       " 'be',\n",
       " 'compressed',\n",
       " 'or',\n",
       " 'any',\n",
       " 'proprietary',\n",
       " 'form',\n",
       " 'including',\n",
       " 'any',\n",
       " 'form',\n",
       " 'resulting',\n",
       " 'from',\n",
       " 'word',\n",
       " 'processing',\n",
       " 'or',\n",
       " 'hypertext',\n",
       " 'software',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'either',\n",
       " '1',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28020a",
   "metadata": {},
   "source": [
    "## Evaluate Models Using Perplexity\n",
    "\n",
    "We evaluate the trained models using the perplexity metric, which measures how well a language model predicts a sequence of words in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f893544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity: 1184.4445469705927\n",
      "Bigram Perplexity : 3352.331453663995\n",
      "Trigram Perplexity: 14283.12273454804\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Perplexity\n",
    "\n",
    "print(\"Unigram Perplexity:\", unigram.perplexity(test_tokens))\n",
    "print(\"Bigram Perplexity :\", bigram.perplexity(test_tokens))\n",
    "print(\"Trigram Perplexity:\", trigram.perplexity(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f149d",
   "metadata": {},
   "source": [
    "## Generate Sentences via Sampling\n",
    "\n",
    "This function uses probabilistic sampling to generate new sentences from a trained N-gram model.\n",
    "\n",
    "**generate_sentence(model, max_len=20):**\n",
    "\n",
    "- Initializes the sentence with `<s>` tokens (length depends on n).\n",
    "\n",
    "- At each step:\n",
    "\n",
    "    - Takes the current context.\n",
    "    - Samples the next word based on the learned n-gram probabilities.\n",
    "    - Stops when it reaches the `</s>` token or max length.\n",
    "    - Returns the generated sentence (excluding the initial `<s>` tokens).\n",
    "\n",
    "Note:\n",
    "- If the context is unseen, the model samples a word randomly from the vocabulary.\n",
    "- For known contexts, it performs weighted random sampling based on frequency.\n",
    "\n",
    "This method mimics how real language is produced: word-by-word, conditioned on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "334c16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, max_len=20):\n",
    "    sentence = ['<s>'] * (model.n - 1)  # Context initialization\n",
    "    for _ in range(max_len):\n",
    "        context = tuple(sentence[-(model.n - 1):]) if model.n > 1 else tuple()\n",
    "        next_words = model.ngram_counts[context]\n",
    "\n",
    "        # If we’ve never seen the context, sample from whole vocab\n",
    "        if not next_words:\n",
    "            next_word = random.choice(list(model.vocab))\n",
    "        else:\n",
    "            # Weighted random sampling\n",
    "            words, counts = zip(*next_words.items())\n",
    "            total = sum(counts)\n",
    "            probs = [c / total for c in counts]\n",
    "            next_word = random.choices(words, weights=probs)[0]\n",
    "\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        sentence.append(next_word)\n",
    "\n",
    "    return ' '.join(sentence[model.n - 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cde9b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- UNIGRAM SAMPLES ---\n",
      "when wish second download hall lay me out page find her disguis her of do my fearful is prison fools\n",
      "thy dies and whole only wrong overdone nails of o and to grace of swaggerer kinsmen s the suffolk venetian\n",
      "other exit i me have sir dare daughter need pound an the field wherefore work isabella dear why himself d\n",
      "then give darkness and the the tell senator have are on ambassador still from to hold his morn so her\n",
      "have his if we madam shall good mother have promise poor to you without your to like triumph dozen to\n",
      "\n",
      "--- BIGRAM SAMPLES ---\n",
      "this a touch me to enter page that nourishes our very very cunning hides wrongs are to speak like a\n",
      "this fellow to passion ay my troth not beg the rank our princes lie upon our judgment to shield thee\n",
      "this resolution whereso er the murmuring lips that which i do not death and since you not he reports go\n",
      "this hair dabbled in christendom shall ne er the law to his gown big for us lartius between master brook\n",
      "this rest and be an honest to mend all his pleasure be distributed so why are masters that i banish\n",
      "\n",
      "--- TRIGRAM SAMPLES ---\n",
      "this is my mother pardon page now master ford are you mankind volumnia ay fool is this expedition austria by\n",
      "this is an arrant knave on my life provost pardon me drinks king aside o knowledge ill inhabited worse than\n",
      "this is the fruits of my new trothed lord urs and did retire to calais without impeachment for to speak\n",
      "this is a naughty world nerissa when the way and to conclude the shepherd knows not well when he number\n",
      "this is yet mine own person answer thy abuse cardinal aside to poins within francis fran my lord but what\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- UNIGRAM SAMPLES ---\")\n",
    "for _ in range(5):\n",
    "    print(generate_sentence(unigram))\n",
    "\n",
    "print(\"\\n--- BIGRAM SAMPLES ---\")\n",
    "for _ in range(5):\n",
    "    print(generate_sentence(bigram))\n",
    "\n",
    "print(\"\\n--- TRIGRAM SAMPLES ---\")\n",
    "for _ in range(5):\n",
    "    print(generate_sentence(trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606528eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
